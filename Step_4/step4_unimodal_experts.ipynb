{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2247dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Step 4.\n",
    "# You can re-run this cell if you create a new environment.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core libraries\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / training utilities\n",
    "%pip install transformers datasets webdataset accelerate timm sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train Unimodal Experts (Text & Image)\\n\n",
    "\\n\n",
    "This notebook uses the packed shards from **Step 3** to train:\\n\n",
    "\\n\n",
    "- A **text-only expert** over the combined text (OCR + captions).\\n\n",
    "- An **image-only expert** over meme images.\\n\n",
    "\\n\n",
    "You can start with the 50-example pilot shards created in Step 3 and later point this\\n\n",
    "notebook at larger shards once you pack the full datasets.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067f370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import webdataset as wds\n",
    "import json\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "\n",
    "# Detect project root so this works whether you start Jupyter in the repo root\n",
    "# or from inside Step_4/.\n",
    "cwd = Path.cwd().resolve()\n",
    "if (cwd / \"Step_3\").is_dir():\n",
    "    root = cwd\n",
    "else:\n",
    "    root = cwd.parent\n",
    "\n",
    "step3 = root / \"Step_3\"\n",
    "shards_dir = step3 / \"shards\" / \"train\"\n",
    "\n",
    "# For the 50-example pilot, we expect a single shard here\n",
    "shard_pattern = str(shards_dir / \"shard-000000.tar\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model names (can be adjusted later)\n",
    "TEXT_MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "VISION_MODEL_NAME = \"google/siglip-base-patch16-384\"  # or another SigLIP-compatible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you have not installed the required libraries yet, run the following\\n\n",
    "> in a separate cell (uncomment as needed):\\n\n",
    "\\n\n",
    "> ```bash\\n\n",
    "> pip install torch torchvision torchaudio\\n\n",
    "> pip install transformers datasets webdataset accelerate timm sentencepiece\\n\n",
    "> ```\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b3df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 text examples and 50 image examples.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "def make_text_dataset(shard_pattern: str, max_samples: Optional[int] = None):\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .to_tuple(\"txt\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, meta_obj in ds:\n",
    "        # text: bytes or str\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        # meta: dict or bytes\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "        out.append({\"text\": text, \"label\": int(y)})\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_image_dataset(shard_pattern: str, max_samples: Optional[int] = None):\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for img, meta_obj in ds:\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "        out.append({\"image\": img, \"label\": int(y)})\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "text_examples = make_text_dataset(shard_pattern, max_samples=1000)\n",
    "image_examples = make_image_dataset(shard_pattern, max_samples=1000)\n",
    "print(f\"Loaded {len(text_examples)} text examples and {len(image_examples)} image examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872f7468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ff2c96f4c94884ad21465622651443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2371ab00d46849e09d697e6f4533fa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d274429b4fdd4edfaed5268fb7537047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da540df82df549d2918af1a2536a4530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db6a27e427d4d7582e87cd8425e2174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text] Epoch 1/1 - loss: 0.6173\n",
      "Saved text expert to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/text_expert\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        enc[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return enc\n",
    "\n",
    "\n",
    "# Initialize tokenizer and model for text expert\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "model_text = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEXT_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "model_text.to(device)\n",
    "\n",
    "# Use all loaded text examples as training data for this pilot\n",
    "dataset_text = TextDataset(text_examples, tokenizer)\n",
    "loader_text = DataLoader(dataset_text, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_text.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 1  # pilot run; can be increased later\n",
    "\n",
    "model_text.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for batch in loader_text:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model_text(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Text] Epoch {epoch + 1}/{EPOCHS} - loss: {total_loss / len(loader_text):.4f}\")\n",
    "\n",
    "# Save text expert\n",
    "models_dir = root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "model_text.save_pretrained(models_dir / \"text_expert\")\n",
    "tokenizer.save_pretrained(models_dir / \"text_expert\")\n",
    "print(\"Saved text expert to\", models_dir / \"text_expert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b4dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of SiglipForImageClassification were not initialized from the model checkpoint at google/siglip-base-patch16-384 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vision] Epoch 1/1 - loss: 0.7791\n",
      "Saved vision expert to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/vision_expert\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]], image_processor, train: bool = True):\n",
    "        self.data = data\n",
    "        self.image_processor = image_processor\n",
    "        self.train = train\n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        if self.train:\n",
    "            img = self.aug(img)\n",
    "        inputs = self.image_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize image processor and model for vision expert\n",
    "image_processor = AutoImageProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "model_vision = AutoModelForImageClassification.from_pretrained(\n",
    "    VISION_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "model_vision.to(device)\n",
    "\n",
    "# Dataset and loader for image examples\n",
    "dataset_img = ImageDataset(image_examples, image_processor, train=True)\n",
    "loader_img = DataLoader(dataset_img, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer_v = torch.optim.AdamW(model_vision.parameters(), lr=1e-4)\n",
    "criterion_v = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_V = 1  # pilot\n",
    "\n",
    "model_vision.train()\n",
    "for epoch in range(EPOCHS_V):\n",
    "    total_loss = 0.0\n",
    "    for batch in loader_img:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model_vision(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion_v(logits, labels)\n",
    "        optimizer_v.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_v.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Vision] Epoch {epoch + 1}/{EPOCHS_V} - loss: {total_loss / len(loader_img):.4f}\")\n",
    "\n",
    "# Save vision expert\n",
    "models_dir = root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "model_vision.save_pretrained(models_dir / \"vision_expert\")\n",
    "image_processor.save_pretrained(models_dir / \"vision_expert\")\n",
    "print(\"Saved vision expert to\", models_dir / \"vision_expert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
