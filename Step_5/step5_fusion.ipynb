{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Multimodal Fusion (Pilot)\n",
    "\n",
    "This notebook trains a **small multimodal fusion model** on the packed shards from **Step 3**, reusing the **text expert** and **vision expert** trained in **Step 4**.\n",
    "\n",
    "For now we run on the 50-example pilot shard (Hateful Memes) to validate the full pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Step 5 (run once per environment).\n",
    "# You can skip this cell if everything is already installed.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core libraries\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / training utilities\n",
    "%pip install transformers datasets webdataset accelerate timm sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Shard pattern: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_3/shards/train/shard-000000.tar\n",
      "Text expert dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/text_expert\n",
      "Vision expert dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/vision_expert\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "from transformers import AutoTokenizer, AutoImageProcessor, AutoModel, AutoModelForImageClassification\n",
    "\n",
    "# Detect project root so this works whether you start Jupyter in the repo root\n",
    "# or from inside Step_5/.\n",
    "cwd = Path.cwd().resolve()\n",
    "if (cwd / \"Step_3\").is_dir():\n",
    "    root = cwd\n",
    "else:\n",
    "    root = cwd.parent\n",
    "\n",
    "step3 = root / \"Step_3\"\n",
    "shards_dir = step3 / \"shards\" / \"train\"\n",
    "shard_pattern = str(shards_dir / \"shard-000000.tar\")  # 50-example pilot shard\n",
    "\n",
    "models_root = root / \"models\"\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"Shard pattern:\", shard_pattern)\n",
    "print(\"Text expert dir:\", text_expert_dir)\n",
    "print(\"Vision expert dir:\", vision_expert_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 multimodal examples.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "\n",
    "def make_fusion_examples(shard_pattern: str, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create a list of {text, image, label} from WebDataset shards.\n",
    "\n",
    "    Uses:\n",
    "    - '<id>.txt'  for combined text\n",
    "    - '<id>.png'  for image (already resized in Step 3)\n",
    "    - '<id>.json' for labels (expects labels.abuse_hate in {0,1})\n",
    "    \"\"\"\n",
    "\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"txt\", \"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, img, meta_obj in ds:\n",
    "        # text may be bytes or str\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        # meta may be dict or bytes\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            \"text\": text,\n",
    "            \"image\": img,\n",
    "            \"label\": int(y),\n",
    "        })\n",
    "\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "fusion_examples = make_fusion_examples(shard_pattern, max_samples=1000)\n",
    "print(f\"Loaded {len(fusion_examples)} multimodal examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 7\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict[str, Any]],\n",
    "        text_tokenizer,\n",
    "        image_processor,\n",
    "        max_length: int = 256,\n",
    "        train: bool = True,\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        self.train = train\n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        if self.train:\n",
    "            img = self.aug(img)\n",
    "\n",
    "        text_enc = self.text_tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_enc = {k: v.squeeze(0) for k, v in text_enc.items()}\n",
    "\n",
    "        img_enc = self.image_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = img_enc[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_enc[\"input_ids\"],\n",
    "            \"attention_mask\": text_enc[\"attention_mask\"],\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load tokenizers / processors from unimodal experts\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_expert_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_expert_dir)\n",
    "\n",
    "dataset_fusion = FusionDataset(\n",
    "    fusion_examples,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    max_length=256,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "loader_fusion = DataLoader(dataset_fusion, batch_size=8, shuffle=True)\n",
    "print(\"Batches per epoch:\", len(loader_fusion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature dim: 768\n",
      "Vision feature dim: 2\n",
      "[Fusion] Epoch 1/1 - loss: 0.4944 - acc: 0.840\n",
      "Saved fusion model weights to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/mm_fusion/fusion_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Build fusion model on top of frozen text and vision encoders\n",
    "\n",
    "# Load encoders from the fine-tuned unimodal experts\n",
    "text_encoder = AutoModel.from_pretrained(text_expert_dir)\n",
    "vision_encoder = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "\n",
    "text_encoder.to(device)\n",
    "vision_encoder.to(device)\n",
    "\n",
    "# Freeze encoder parameters for this pilot (only train fusion head)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in vision_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Determine feature dimensions\n",
    "# Text: use hidden_size from the backbone\n",
    "t_dim = text_encoder.config.hidden_size\n",
    "# Vision: we will use the classifier logits as features (dim = num_labels)\n",
    "v_dim = vision_encoder.config.num_labels\n",
    "\n",
    "print(\"Text feature dim:\", t_dim)\n",
    "print(\"Vision feature dim:\", v_dim)\n",
    "\n",
    "fusion_hidden = 512\n",
    "num_labels = 2\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: nn.Module,\n",
    "        vision_encoder: nn.Module,\n",
    "        t_dim: int,\n",
    "        v_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim + v_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Extract CLS / pooled representations without updating encoder weights\n",
    "        with torch.no_grad():\n",
    "            text_out = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            # Prefer pooler_output if available, else CLS token\n",
    "            if hasattr(text_out, \"pooler_output\") and text_out.pooler_output is not None:\n",
    "                t_repr = text_out.pooler_output\n",
    "            else:\n",
    "                t_repr = text_out.last_hidden_state[:, 0, :]\n",
    "\n",
    "            # For the vision expert, use classification logits as a compact feature\n",
    "            vision_out = self.vision_encoder(pixel_values=pixel_values)\n",
    "            v_repr = vision_out.logits\n",
    "\n",
    "        h = torch.cat([t_repr, v_repr], dim=-1)\n",
    "        logits = self.mlp(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "fusion_model = FusionModel(\n",
    "    text_encoder=text_encoder,\n",
    "    vision_encoder=vision_encoder,\n",
    "    t_dim=t_dim,\n",
    "    v_dim=v_dim,\n",
    "    hidden_dim=fusion_hidden,\n",
    "    num_labels=num_labels,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 1  # pilot run\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fusion_model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader_fusion:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "\n",
    "        logits = fusion_model(**batch)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader_fusion))\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Fusion] Epoch {epoch + 1}/{EPOCHS} - loss: {avg_loss:.4f} - acc: {acc:.3f}\")\n",
    "\n",
    "# Save fusion model weights (pilot artifact)\n",
    "mm_dir = models_root / \"mm_fusion\"\n",
    "mm_dir.mkdir(exist_ok=True)\n",
    "fusion_path = mm_dir / \"fusion_model.pt\"\n",
    "torch.save(fusion_model.state_dict(), fusion_path)\n",
    "print(\"Saved fusion model weights to\", fusion_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
