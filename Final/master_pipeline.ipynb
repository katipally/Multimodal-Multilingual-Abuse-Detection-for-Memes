{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef127904",
   "metadata": {},
   "source": [
    "# Multimodal Hate/Abuse Detection — Master Pipeline (Steps 2–8)\n",
    "\n",
    "This notebook orchestrates the **entire pipeline** from Step 2 to Step 8\n",
    "for the multimodal hate/abuse detection system.\n",
    "\n",
    "It is designed to be **adaptive**:\n",
    "- You can run it on the **pilot subset** or the **full dataset**.\n",
    "- All intermediate artifacts for this notebook are written under\n",
    "  `Final/` (except **models**, which always live under `models/`).\n",
    "- Each step is clearly separated so you can re-run only parts as needed.\n",
    "\n",
    "> **Note:** This notebook *wraps* the existing scripts/notebooks\n",
    "> (`Step_2`, `Step_3`, `Step_4`, `Step_5`, `Step_6`, `Step_7`, `Step_8`).\n",
    "> If you want to debug internals of a specific step, you can still open\n",
    "> the original step notebooks/scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8e85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: paddleocr in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: paddlepaddle in /opt/anaconda3/lib/python3.12/site-packages (3.2.2)\n",
      "Requirement already satisfied: paddlex<3.4.0,>=3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.3.10)\n",
      "Requirement already satisfied: PyYAML>=6 in /opt/anaconda3/lib/python3.12/site-packages (from paddleocr) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from paddleocr) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from paddleocr) (4.14.1)\n",
      "Requirement already satisfied: aistudio-sdk>=0.3.5 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.3.8)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.2.0)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (6.9.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.36.0)\n",
      "Requirement already satisfied: modelscope>=1.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.32.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (23.2)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.3.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (10.4.0)\n",
      "Requirement already satisfied: prettytable in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.16.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.12.3)\n",
      "Requirement already satisfied: ruamel.yaml in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.18.6)\n",
      "Requirement already satisfied: ujson in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.10.0)\n",
      "Requirement already satisfied: imagesize in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.4.1)\n",
      "Requirement already satisfied: opencv-contrib-python==4.10.0.84 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.10.0.84)\n",
      "Requirement already satisfied: pyclipper in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.3.0.post6)\n",
      "Requirement already satisfied: pypdfium2>=4 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.30.0)\n",
      "Requirement already satisfied: python-bidi in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.6.7)\n",
      "Requirement already satisfied: shapely in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.1.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (0.27.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (4.25.3)\n",
      "Requirement already satisfied: opt_einsum==3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (3.3)\n",
      "Requirement already satisfied: safetensors>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (0.7.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.67.1)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.9.54)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (8.2.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from modelscope>=1.28.0->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (75.8.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from modelscope>=1.28.0->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->paddleocr) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->paddleocr) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->paddleocr) (2025.8.3)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from bce-python-sdk->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.23.0)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from bce-python-sdk->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.0.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->paddlepaddle) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->paddlepaddle) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->paddlepaddle) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prettytable->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.2.5)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/anaconda3/lib/python3.12/site-packages (from ruamel.yaml->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /opt/anaconda3/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (1.2.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.122.0)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.0.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.12.3)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages for the full pipeline (run once per environment).\n",
    "# You can skip this cell if everything is already installed.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core + training libs\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / datasets / utilities\n",
    "%pip install transformers datasets webdataset accelerate timm sentencepiece pillow\n",
    "\n",
    "# OCR stack for Step 2\n",
    "%pip install paddleocr paddlepaddle\n",
    "\n",
    "# Lightweight web UI for Step 8\n",
    "%pip install \"huggingface-hub>=0.33.5,<1.0\" gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384a91d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yashwanthreddy/Documents/GitHub/DL_Proj\n",
      "Datasets root: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets\n",
      "Final outputs root: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final\n",
      "Using torch device: cpu\n",
      "sys.path includes project root: True\n"
     ]
    }
   ],
   "source": [
    "# Global configuration and paths\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Robust project root detection:\n",
    "# Look for the Datasets/ folder which only exists at the true project root\n",
    "ROOT = Path.cwd().resolve()\n",
    "\n",
    "# Walk up until we find the actual project root (contains Datasets/)\n",
    "project_root = ROOT\n",
    "for _ in range(5):  # max 5 levels up\n",
    "    if (project_root / \"Datasets\").is_dir():\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "\n",
    "# Sanity check\n",
    "if not (project_root / \"Datasets\").is_dir():\n",
    "    raise RuntimeError(f\"Could not find project root with Datasets/ folder. Searched from {ROOT}\")\n",
    "\n",
    "FINAL_DIR = project_root / \"Final\"\n",
    "FINAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Per-step output roots for this master notebook\n",
    "FINAL_STEP2 = FINAL_DIR / \"Step_2\"\n",
    "FINAL_STEP3 = FINAL_DIR / \"Step_3\"\n",
    "FINAL_STEP6 = FINAL_DIR / \"Step_6\"\n",
    "FINAL_STEP7 = FINAL_DIR / \"Step_7\"\n",
    "\n",
    "for p in [FINAL_STEP2, FINAL_STEP3, FINAL_STEP6, FINAL_STEP7]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Root directory where datasets are stored.\n",
    "# Override this if your datasets live somewhere else.\n",
    "DATASETS_ROOT = project_root / \"Datasets\"\n",
    "\n",
    "if not DATASETS_ROOT.exists():\n",
    "    raise RuntimeError(f\"DATASETS_ROOT does not exist: {DATASETS_ROOT}\")\n",
    "\n",
    "# Global torch device: GPU first (CUDA), then CPU fallback.\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# For backwards compatibility with later cells that expect `device`\n",
    "device = DEVICE\n",
    "\n",
    "# Ensure project_root is importable if you ever need to import local modules\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Datasets root:\", DATASETS_ROOT)\n",
    "print(\"Final outputs root:\", FINAL_DIR)\n",
    "print(\"Using torch device:\", DEVICE)\n",
    "print(\"sys.path includes project root:\", str(project_root) in sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1a453",
   "metadata": {},
   "source": [
    "## Step 2 — Build Manifest (Final/Step_2/data_manifest.jsonl)\n",
    "\n",
    "This section wraps `Step_2/build_data_manifest.py` and writes the\n",
    "manifest for this master notebook under `Final/Step_2/data_manifest.jsonl`.\n",
    "\n",
    "The manifest still reads raw datasets from `Datasets/`, but **no files\n",
    "are written into `Step_2/`** by this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9e4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building manifest at: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_2/data_manifest.jsonl\n",
      "\n",
      "First 3 lines of manifest:\n",
      "{\"id\": \"memotion1_image_1.jpg\", \"dataset\": \"memotion1\", \"split\": \"train\", \"image_path\": \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets/memotion1/images/image_1.jpg\", \"text_raw\": \"LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIKUT TREND PLAY THE 10 YEARS CHALLENGE AT FACEBOOK imgflip.com\", \"labels_raw\": {\"humour\": \"hilarious\", \"sarcasm\": \"general\", \"offensive\": \"not_offensive\", \"motivational\": \"not_motivational\", \"overall_sentiment\": \"very_positive\"}}\n",
      "{\"id\": \"memotion1_image_2.jpeg\", \"dataset\": \"memotion1\", \"split\": \"train\", \"image_path\": \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets/memotion1/images/image_2.jpeg\", \"text_raw\": \"The best of #10 YearChallenge! Completed in less the 4 years. Kudus to @narendramodi ji 8:05 PM - 16 Jan 2019 from Mumbai  India\", \"labels_raw\": {\"humour\": \"not_funny\", \"sarcasm\": \"general\", \"offensive\": \"not_offensive\", \"motivational\": \"motivational\", \"overall_sentiment\": \"very_positive\"}}\n",
      "{\"id\": \"memotion1_image_3.JPG\", \"dataset\": \"memotion1\", \"split\": \"train\", \"image_path\": \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets/memotion1/images/image_3.JPG\", \"text_raw\": \"Sam Thorne @Strippin ( Follow Follow Saw everyone posting these 2009 vs 2019 pics so here's mine 6:23 PM - 12 Jan 2019 O 636 Retweets 3 224 LIKES 65 636 3.2K\", \"labels_raw\": {\"humour\": \"very_funny\", \"sarcasm\": \"not_sarcastic\", \"offensive\": \"not_offensive\", \"motivational\": \"not_motivational\", \"overall_sentiment\": \"positive\"}}\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Build Manifest (inlined from Step_2/build_data_manifest.py)\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "# Datasets live under the configurable root\n",
    "DATASETS = DATASETS_ROOT\n",
    "\n",
    "\n",
    "def iter_hateful_memes() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"hateful_memes\"\n",
    "    for split_name, filename in [\n",
    "        (\"train\", \"train.jsonl\"),\n",
    "        (\"dev\", \"dev.jsonl\"),\n",
    "        (\"test\", \"test.jsonl\"),\n",
    "    ]:\n",
    "        jsonl_path = ds_root / filename\n",
    "        if not jsonl_path.exists():\n",
    "            continue\n",
    "        with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                img_rel = obj.get(\"img\")\n",
    "                image_path = str((ds_root / img_rel).resolve()) if img_rel else None\n",
    "                yield {\n",
    "                    \"id\": f\"hateful_memes_{split_name}_{obj.get('id')}\",\n",
    "                    \"dataset\": \"hateful_memes\",\n",
    "                    \"split\": split_name,\n",
    "                    \"image_path\": image_path,\n",
    "                    \"text_raw\": obj.get(\"text\", \"\"),\n",
    "                    \"labels_raw\": obj.get(\"label\"),\n",
    "                }\n",
    "\n",
    "\n",
    "def iter_mami() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"mami\"\n",
    "\n",
    "    # Train: TRAINING/training.csv + images\n",
    "    train_csv = ds_root / \"TRAINING\" / \"training.csv\"\n",
    "    train_img_root: Optional[Path] = None\n",
    "    training_dir = ds_root / \"TRAINING\"\n",
    "    # Heuristic: images are either directly under TRAINING or a subfolder named like \"training\" / \"Training\".\n",
    "    if training_dir.exists():\n",
    "        # Prefer nested image folder if present\n",
    "        for sub in training_dir.iterdir():\n",
    "            if sub.is_dir() and sub.name.lower().startswith(\"train\"):\n",
    "                train_img_root = sub\n",
    "                break\n",
    "        if train_img_root is None:\n",
    "            train_img_root = training_dir\n",
    "\n",
    "    if train_csv.exists() and train_img_root is not None:\n",
    "        with train_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "            # If delimiter guess fails, fall back to default csv dialect\n",
    "            if reader.fieldnames is None or reader.fieldnames == [\n",
    "                \"file_name,misogynous,shaming,stereotype,objectification,violence,Text Transcription\"\n",
    "            ]:\n",
    "                f.seek(0)\n",
    "                reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                file_name = row.get(\"file_name\") or row.get(\"file_name \")\n",
    "                if not file_name:\n",
    "                    continue\n",
    "                img_path = (train_img_root / file_name).resolve()\n",
    "                text = row.get(\"Text Transcription\", \"\")\n",
    "                labels = {\n",
    "                    \"misogynous\": int(row.get(\"misogynous\", 0)),\n",
    "                    \"shaming\": int(row.get(\"shaming\", 0)),\n",
    "                    \"stereotype\": int(row.get(\"stereotype\", 0)),\n",
    "                    \"objectification\": int(row.get(\"objectification\", 0)),\n",
    "                    \"violence\": int(row.get(\"violence\", 0)),\n",
    "                }\n",
    "                yield {\n",
    "                    \"id\": f\"mami_train_{file_name}\",\n",
    "                    \"dataset\": \"mami\",\n",
    "                    \"split\": \"train\",\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"text_raw\": text,\n",
    "                    \"labels_raw\": labels,\n",
    "                }\n",
    "\n",
    "    # Test: test/Test.csv + test_labels.txt + images\n",
    "    test_txt = ds_root / \"test_labels.txt\"\n",
    "    test_csv = ds_root / \"test\" / \"Test.csv\"\n",
    "    test_img_root: Optional[Path] = None\n",
    "    test_dir = ds_root / \"test\"\n",
    "    if test_dir.exists():\n",
    "        test_img_root = test_dir\n",
    "\n",
    "    labels_by_name: Dict[str, List[int]] = {}\n",
    "    if test_txt.exists():\n",
    "        with test_txt.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) != 6:\n",
    "                    continue\n",
    "                fn = parts[0]\n",
    "                try:\n",
    "                    vals = [int(x) for x in parts[1:]]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                labels_by_name[fn] = vals\n",
    "\n",
    "    if test_csv.exists() and test_img_root is not None:\n",
    "        with test_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                file_name = row.get(\"file_name\")\n",
    "                if not file_name:\n",
    "                    continue\n",
    "                img_path = (test_img_root / file_name).resolve()\n",
    "                text = row.get(\"Text Transcription\", \"\")\n",
    "                raw_labels = labels_by_name.get(file_name)\n",
    "                labels = None\n",
    "                if raw_labels is not None and len(raw_labels) == 5:\n",
    "                    labels = {\n",
    "                        \"misogynous\": raw_labels[0],\n",
    "                        \"shaming\": raw_labels[1],\n",
    "                        \"stereotype\": raw_labels[2],\n",
    "                        \"objectification\": raw_labels[3],\n",
    "                        \"violence\": raw_labels[4],\n",
    "                    }\n",
    "                yield {\n",
    "                    \"id\": f\"mami_test_{file_name}\",\n",
    "                    \"dataset\": \"mami\",\n",
    "                    \"split\": \"test\",\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"text_raw\": text,\n",
    "                    \"labels_raw\": labels,\n",
    "                }\n",
    "\n",
    "\n",
    "def iter_mmhs150k() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"mmhs150k\"\n",
    "    gt_path = ds_root / \"MMHS150K_GT.json\"\n",
    "    splits_root = ds_root / \"splits\"\n",
    "    img_root = ds_root / \"img_resized\"\n",
    "    if not gt_path.exists():\n",
    "        return\n",
    "    with gt_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        gt = json.load(f)\n",
    "\n",
    "    id_to_split: Dict[str, str] = {}\n",
    "    for split_name, fname in [\n",
    "        (\"train\", \"train_ids.txt\"),\n",
    "        (\"val\", \"val_ids.txt\"),\n",
    "        (\"test\", \"test_ids.txt\"),\n",
    "    ]:\n",
    "        p = splits_root / fname\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f_ids:\n",
    "            for line in f_ids:\n",
    "                ex_id = line.strip()\n",
    "                if not ex_id:\n",
    "                    continue\n",
    "                id_to_split[ex_id] = split_name\n",
    "\n",
    "    for ex_id, info in gt.items():\n",
    "        split = id_to_split.get(ex_id, \"unsplit\")\n",
    "        img_name = f\"{ex_id}.jpg\"\n",
    "        img_path = (img_root / img_name).resolve()\n",
    "        text = info.get(\"tweet_text\", \"\")\n",
    "        labels = {\n",
    "            \"labels\": info.get(\"labels\"),\n",
    "            \"labels_str\": info.get(\"labels_str\"),\n",
    "        }\n",
    "        yield {\n",
    "            \"id\": f\"mmhs150k_{ex_id}\",\n",
    "            \"dataset\": \"mmhs150k\",\n",
    "            \"split\": split,\n",
    "            \"image_path\": str(img_path),\n",
    "            \"text_raw\": text,\n",
    "            \"labels_raw\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "def iter_memotion1() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"memotion1\"\n",
    "    csv_path = ds_root / \"labels.csv\"\n",
    "    img_root = ds_root / \"images\"\n",
    "    if not csv_path.exists():\n",
    "        return\n",
    "\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            img_name = row.get(\"image_name\")\n",
    "            if not img_name:\n",
    "                continue\n",
    "            img_path = (img_root / img_name).resolve()\n",
    "            text = row.get(\"text_corrected\") or row.get(\"text_ocr\") or \"\"\n",
    "            labels = {\n",
    "                \"humour\": row.get(\"humour\"),\n",
    "                \"sarcasm\": row.get(\"sarcasm\"),\n",
    "                \"offensive\": row.get(\"offensive\"),\n",
    "                \"motivational\": row.get(\"motivational\"),\n",
    "                \"overall_sentiment\": row.get(\"overall_sentiment\"),\n",
    "            }\n",
    "            yield {\n",
    "                \"id\": f\"memotion1_{img_name}\",\n",
    "                \"dataset\": \"memotion1\",\n",
    "                \"split\": \"train\",  # dataset does not define explicit splits\n",
    "                \"image_path\": str(img_path),\n",
    "                \"text_raw\": text,\n",
    "                \"labels_raw\": labels,\n",
    "            }\n",
    "\n",
    "\n",
    "def iter_memotion2() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"memotion2\"\n",
    "    img_root_train = ds_root / \"image folder\" / \"train_images\"\n",
    "    img_root_val = ds_root / \"image folder\" / \"val_images\"\n",
    "\n",
    "    def _iter_split(csv_path: Path, img_root: Path, split_name: str) -> Iterable[Dict[str, Any]]:\n",
    "        if not csv_path.exists() or not img_root.exists():\n",
    "            return []\n",
    "        with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                ex_id = row.get(\"Id\")\n",
    "                if not ex_id:\n",
    "                    continue\n",
    "                img_name = f\"{ex_id}.jpg\"\n",
    "                img_path = (img_root / img_name).resolve()\n",
    "                text = row.get(\"ocr_text\", \"\")\n",
    "                labels = {\n",
    "                    \"humour\": row.get(\"humour\"),\n",
    "                    \"sarcastic\": row.get(\"sarcastic\"),\n",
    "                    \"offensive\": row.get(\"offensive\"),\n",
    "                    \"motivational\": row.get(\"motivational\"),\n",
    "                    \"overall_sentiment\": row.get(\"overall_sentiment\"),\n",
    "                    \"classification_based_on\": row.get(\"classification_based_on\"),\n",
    "                }\n",
    "                yield {\n",
    "                    \"id\": f\"memotion2_{split_name}_{ex_id}\",\n",
    "                    \"dataset\": \"memotion2\",\n",
    "                    \"split\": split_name,\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"text_raw\": text,\n",
    "                    \"labels_raw\": labels,\n",
    "                }\n",
    "\n",
    "    train_csv = ds_root / \"Memotion2\" / \"memotion_train.csv\"\n",
    "    val_csv = ds_root / \"Memotion2\" / \"memotion_val.csv\"\n",
    "\n",
    "    for rec in _iter_split(train_csv, img_root_train, \"train\"):\n",
    "        yield rec\n",
    "    for rec in _iter_split(val_csv, img_root_val, \"val\"):\n",
    "        yield rec\n",
    "\n",
    "    # Test split has no labels; include if desired\n",
    "    test_csv = ds_root / \"memotion_test.csv\"\n",
    "    if test_csv.exists():\n",
    "        with test_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                ex_id = row.get(\"Id\")\n",
    "                if not ex_id:\n",
    "                    continue\n",
    "                img_name = f\"{ex_id}.jpg\"\n",
    "                # Test images are usually not provided locally; leave image_path empty if missing\n",
    "                img_path = None\n",
    "                if img_root_train.exists():\n",
    "                    candidate = img_root_train / img_name\n",
    "                    if candidate.exists():\n",
    "                        img_path = str(candidate.resolve())\n",
    "                if img_path is None and img_root_val.exists():\n",
    "                    candidate = img_root_val / img_name\n",
    "                    if candidate.exists():\n",
    "                        img_path = str(candidate.resolve())\n",
    "                text = row.get(\"ocr_text\", \"\")\n",
    "                labels = {\n",
    "                    \"classification_based_on\": row.get(\"classification_based_on\"),\n",
    "                }\n",
    "                yield {\n",
    "                    \"id\": f\"memotion2_test_{ex_id}\",\n",
    "                    \"dataset\": \"memotion2\",\n",
    "                    \"split\": \"test\",\n",
    "                    \"image_path\": img_path,\n",
    "                    \"text_raw\": text,\n",
    "                    \"labels_raw\": labels,\n",
    "                }\n",
    "\n",
    "\n",
    "def iter_hatexplain() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"hatexplain\" / \"Data\"\n",
    "    dataset_path = ds_root / \"dataset.json\"\n",
    "    splits_path = ds_root / \"post_id_divisions.json\"\n",
    "    if not dataset_path.exists() or not splits_path.exists():\n",
    "        return\n",
    "    with dataset_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    with splits_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        splits = json.load(f)\n",
    "\n",
    "    id_to_split: Dict[str, str] = {}\n",
    "    for split_name, ids in splits.items():\n",
    "        for pid in ids:\n",
    "            id_to_split[pid] = split_name\n",
    "\n",
    "    for pid, info in data.items():\n",
    "        split = id_to_split.get(pid, \"unsplit\")\n",
    "        tokens = info.get(\"post_tokens\", [])\n",
    "        text = \" \".join(tokens)\n",
    "        labels = info.get(\"annotators\", [])\n",
    "        yield {\n",
    "            \"id\": f\"hatexplain_{pid}\",\n",
    "            \"dataset\": \"hatexplain\",\n",
    "            \"split\": split,\n",
    "            \"image_path\": None,\n",
    "            \"text_raw\": text,\n",
    "            \"labels_raw\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "def iter_olid() -> Iterable[Dict[str, Any]]:\n",
    "    ds_root = DATASETS / \"olid\"\n",
    "    train_tsv = ds_root / \"olid-training-v1.0.tsv\"\n",
    "    test_tsv = ds_root / \"testset-levela.tsv\"\n",
    "    test_labels_csv = ds_root / \"labels-levela.csv\"\n",
    "\n",
    "    # Train\n",
    "    if train_tsv.exists():\n",
    "        with train_tsv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                ex_id = row.get(\"id\")\n",
    "                if not ex_id:\n",
    "                    continue\n",
    "                text = row.get(\"tweet\", \"\")\n",
    "                labels = {\n",
    "                    \"subtask_a\": row.get(\"subtask_a\"),\n",
    "                    \"subtask_b\": row.get(\"subtask_b\"),\n",
    "                    \"subtask_c\": row.get(\"subtask_c\"),\n",
    "                }\n",
    "                yield {\n",
    "                    \"id\": f\"olid_train_{ex_id}\",\n",
    "                    \"dataset\": \"olid\",\n",
    "                    \"split\": \"train\",\n",
    "                    \"image_path\": None,\n",
    "                    \"text_raw\": text,\n",
    "                    \"labels_raw\": labels,\n",
    "                }\n",
    "\n",
    "    # Test (level a)\n",
    "    id_to_label: Dict[str, str] = {}\n",
    "    if test_labels_csv.exists():\n",
    "        with test_labels_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if len(row) != 2:\n",
    "                    continue\n",
    "                id_to_label[row[0]] = row[1]\n",
    "\n",
    "    if test_tsv.exists():\n",
    "        with test_tsv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                ex_id = row.get(\"id\")\n",
    "                if not ex_id:\n",
    "                    continue\n",
    "                text = row.get(\"tweet\", \"\")\n",
    "                label = id_to_label.get(ex_id)\n",
    "                labels = {\"subtask_a\": label}\n",
    "                yield {\n",
    "                    \"id\": f\"olid_test_{ex_id}\",\n",
    "                    \"dataset\": \"olid\",\n",
    "                    \"split\": \"test\",\n",
    "                    \"image_path\": None,\n",
    "                    \"text_raw\": text,\n",
    "                    \"labels_raw\": labels,\n",
    "                }\n",
    "\n",
    "\n",
    "def build_manifest_step2(output_path: Path) -> None:\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as out_f:\n",
    "        for iterator in [\n",
    "            iter_hateful_memes,\n",
    "            iter_mami,\n",
    "            iter_mmhs150k,\n",
    "            iter_memotion1,\n",
    "            iter_memotion2,\n",
    "            iter_hatexplain,\n",
    "            iter_olid,\n",
    "        ]:\n",
    "            for record in iterator():\n",
    "                out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# Run manifest build for this master pipeline\n",
    "manifest_path = FINAL_STEP2 / \"data_manifest.jsonl\"\n",
    "print(\"Building manifest at:\", manifest_path)\n",
    "build_manifest_step2(manifest_path)\n",
    "\n",
    "# Quick sanity check: show a few lines\n",
    "print(\"\\nFirst 3 lines of manifest:\")\n",
    "with manifest_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4ea85",
   "metadata": {},
   "source": [
    "## Step 2 — Run OCR (Final/Step_2/ocr.jsonl)\n",
    "\n",
    "Run PaddleOCR over **all images** listed in the manifest and write\n",
    "results to `Final/Step_2/ocr.jsonl`. QC images are written to\n",
    "`Final/Step_2/ocr_qc/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ad92f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running OCR...\n",
      "Loading PaddleOCR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/Users/yashwanthreddy/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('en_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/Users/yashwanthreddy/.paddlex/official_models/en_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddleOCR initialized (no explicit use_gpu; using library defaults).\n",
      "PaddleOCR model loaded.\n",
      "Counting images in manifest...\n",
      "Will process up to 6992 images.\n",
      "  Processed 200/6992 images (2.9%) - 0.6 img/s - ETA: 200.9 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 284\u001b[0m\n\u001b[1;32m    282\u001b[0m ocr_output_path \u001b[38;5;241m=\u001b[39m FINAL_STEP2 \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mocr.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning OCR...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m run_ocr_step2(\n\u001b[1;32m    285\u001b[0m     manifest_path\u001b[38;5;241m=\u001b[39mmanifest_path,\n\u001b[1;32m    286\u001b[0m     output_path\u001b[38;5;241m=\u001b[39mocr_output_path,\n\u001b[1;32m    287\u001b[0m     qc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m     limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    289\u001b[0m     progress_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCR output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ocr_output_path)\n",
      "Cell \u001b[0;32mIn[4], line 227\u001b[0m, in \u001b[0;36mrun_ocr_step2\u001b[0;34m(manifest_path, output_path, qc, limit, progress_interval)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Run OCR\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m lines \u001b[38;5;241m=\u001b[39m _run_ocr_on_image(ocr_engine, img_path)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[1;32m    230\u001b[0m     no_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m, in \u001b[0;36m_run_ocr_on_image\u001b[0;34m(ocr_engine, image_path)\u001b[0m\n\u001b[1;32m    120\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# PaddleOCR 3.x API: use predict() method\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m results \u001b[38;5;241m=\u001b[39m ocr_engine\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimg_array)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lines\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddleocr/_pipelines/ocr.py:213\u001b[0m, in \u001b[0;36mPaddleOCR.predict\u001b[0;34m(self, input, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_rec_score_thresh, return_word_box)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m     return_word_box\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    212\u001b[0m ):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_iter(\n\u001b[1;32m    215\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    216\u001b[0m             use_doc_orientation_classify\u001b[38;5;241m=\u001b[39muse_doc_orientation_classify,\n\u001b[1;32m    217\u001b[0m             use_doc_unwarping\u001b[38;5;241m=\u001b[39muse_doc_unwarping,\n\u001b[1;32m    218\u001b[0m             use_textline_orientation\u001b[38;5;241m=\u001b[39muse_textline_orientation,\n\u001b[1;32m    219\u001b[0m             text_det_limit_side_len\u001b[38;5;241m=\u001b[39mtext_det_limit_side_len,\n\u001b[1;32m    220\u001b[0m             text_det_limit_type\u001b[38;5;241m=\u001b[39mtext_det_limit_type,\n\u001b[1;32m    221\u001b[0m             text_det_thresh\u001b[38;5;241m=\u001b[39mtext_det_thresh,\n\u001b[1;32m    222\u001b[0m             text_det_box_thresh\u001b[38;5;241m=\u001b[39mtext_det_box_thresh,\n\u001b[1;32m    223\u001b[0m             text_det_unclip_ratio\u001b[38;5;241m=\u001b[39mtext_det_unclip_ratio,\n\u001b[1;32m    224\u001b[0m             text_rec_score_thresh\u001b[38;5;241m=\u001b[39mtext_rec_score_thresh,\n\u001b[1;32m    225\u001b[0m             return_word_box\u001b[38;5;241m=\u001b[39mreturn_word_box,\n\u001b[1;32m    226\u001b[0m         )\n\u001b[1;32m    227\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/pipelines/_parallel.py:129\u001b[0m, in \u001b[0;36mAutoParallelSimpleInferencePipeline.predict\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/pipelines/ocr/pipeline.py:441\u001b[0m, in \u001b[0;36m_OCRPipeline.predict\u001b[0;34m(self, input, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_max_side_limit, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_rec_score_thresh, return_word_box)\u001b[0m\n\u001b[1;32m    435\u001b[0m sorted_subs_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    436\u001b[0m     sub_img_info_list, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    437\u001b[0m )\n\u001b[1;32m    438\u001b[0m sorted_subs_of_img \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    439\u001b[0m     all_subs_of_img[x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sorted_subs_info\n\u001b[1;32m    440\u001b[0m ]\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rec_res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_rec_model(\n\u001b[1;32m    443\u001b[0m         sorted_subs_of_img, return_word_box\u001b[38;5;241m=\u001b[39mreturn_word_box\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m ):\n\u001b[1;32m    446\u001b[0m     sub_img_id \u001b[38;5;241m=\u001b[39m sorted_subs_info[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    447\u001b[0m     sub_img_info_list[sub_img_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec_res\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m rec_res\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/models/base/predictor/base_predictor.py:272\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, input, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _apply(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/models/base/predictor/base_predictor.py:329\u001b[0m, in \u001b[0;36mBasePredictor.apply\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m--> 329\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(batch_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    330\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m PredictionWrap(prediction, \u001b[38;5;28mlen\u001b[39m(batch_data))\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_data)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/models/text_recognition/predictor.py:89\u001b[0m, in \u001b[0;36mTextRecPredictor.process\u001b[0;34m(self, batch_data, return_word_box)\u001b[0m\n\u001b[1;32m     87\u001b[0m batch_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReisizeNorm\u001b[39m\u001b[38;5;124m\"\u001b[39m](imgs\u001b[38;5;241m=\u001b[39mbatch_raw_imgs)\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m](imgs\u001b[38;5;241m=\u001b[39mbatch_imgs)\n\u001b[0;32m---> 89\u001b[0m batch_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(x\u001b[38;5;241m=\u001b[39mx)\n\u001b[1;32m     90\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     91\u001b[0m img_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_raw_imgs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/models/common/static_infer.py:297\u001b[0m, in \u001b[0;36mPaddleInfer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    295\u001b[0m x \u001b[38;5;241m=\u001b[39m _sort_inputs(x, names)\n\u001b[1;32m    296\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(np\u001b[38;5;241m.\u001b[39mascontiguousarray, x))\n\u001b[0;32m--> 297\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(x)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/paddlex/inference/models/common/static_infer.py:260\u001b[0m, in \u001b[0;36mPaddleInferChainLegacy.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    258\u001b[0m     input_handle\u001b[38;5;241m.\u001b[39mreshape(input_\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    259\u001b[0m     input_handle\u001b[38;5;241m.\u001b[39mcopy_from_cpu(input_)\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    261\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mcopy_to_cpu() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_handles]\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 2 — Run OCR (inlined from Step_2/run_ocr.py, outputs under Final/Step_2)\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# QC images live under Final/Step_2/ocr_qc\n",
    "QC_DIR = FINAL_STEP2 / \"ocr_qc\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OcrLine:\n",
    "    text: str\n",
    "    conf: float\n",
    "    bbox: List[float]  # [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "def _load_paddleocr():\n",
    "    \"\"\"Load PaddleOCR with the modern 3.x API settings.\n",
    "\n",
    "    We try to initialize with GPU first (use_gpu=True) *if* the installed\n",
    "    PaddleOCR supports that argument. Otherwise we rely on the library's\n",
    "    own device selection (GPU build -> GPU, otherwise CPU).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from paddleocr import PaddleOCR\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\n",
    "            \"PaddleOCR is not installed. Install with:\\n\"\n",
    "            \"  pip install paddleocr paddlepaddle pillow numpy\\n\"\n",
    "        ) from e\n",
    "\n",
    "    base_kwargs = dict(\n",
    "        use_doc_orientation_classify=False,\n",
    "        use_doc_unwarping=False,\n",
    "        use_textline_orientation=False,\n",
    "        lang=\"en\",\n",
    "    )\n",
    "\n",
    "    # Some PaddleOCR versions expose a 'use_gpu' argument, others do not.\n",
    "    # We inspect the signature to decide whether we can pass it.\n",
    "    import inspect\n",
    "\n",
    "    sig = inspect.signature(PaddleOCR.__init__)\n",
    "    supports_use_gpu = \"use_gpu\" in sig.parameters\n",
    "\n",
    "    if supports_use_gpu:\n",
    "        try:\n",
    "            ocr = PaddleOCR(**base_kwargs, use_gpu=True)\n",
    "            print(\"PaddleOCR initialized with GPU (use_gpu=True).\")\n",
    "            return ocr\n",
    "        except Exception as gpu_err:\n",
    "            print(f\"[INFO] PaddleOCR GPU init failed, falling back to CPU: {gpu_err}\")\n",
    "            try:\n",
    "                ocr = PaddleOCR(**base_kwargs, use_gpu=False)\n",
    "                print(\"PaddleOCR initialized on CPU (use_gpu=False).\")\n",
    "                return ocr\n",
    "            except Exception as cpu_err:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to initialize PaddleOCR even on CPU: {cpu_err}\"\n",
    "                ) from cpu_err\n",
    "\n",
    "    # If 'use_gpu' is not supported, fall back to defaults (library decides device)\n",
    "    try:\n",
    "        ocr = PaddleOCR(**base_kwargs)\n",
    "        print(\"PaddleOCR initialized (no explicit use_gpu; using library defaults).\")\n",
    "        return ocr\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize PaddleOCR: {e}\") from e\n",
    "\n",
    "\n",
    "def _iter_manifest_ocr(manifest_path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    \"\"\"Iterate over records in the manifest JSONL file.\"\"\"\n",
    "    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def _count_manifest_images(manifest_path: Path) -> int:\n",
    "    \"\"\"Count records with image_path in the manifest.\"\"\"\n",
    "    count = 0\n",
    "    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            if rec.get(\"image_path\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def _resize_image_if_needed(img: Image.Image, max_side: int = 1024) -> Image.Image:\n",
    "    \"\"\"Resize image so long side <= max_side, preserving aspect ratio.\"\"\"\n",
    "    w, h = img.size\n",
    "    long_side = max(w, h)\n",
    "    if long_side > max_side:\n",
    "        scale = max_side / float(long_side)\n",
    "        new_size = (int(w * scale), int(h * scale))\n",
    "        img = img.resize(new_size, Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _run_ocr_on_image(ocr_engine, image_path: Path) -> List[OcrLine]:\n",
    "    \"\"\"Run OCR on a single image using PaddleOCR 3.x predict() API.\"\"\"\n",
    "    lines: List[OcrLine] = []\n",
    "\n",
    "    try:\n",
    "        # Load and optionally resize image\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = _resize_image_if_needed(img, max_side=1024)\n",
    "            img_array = np.array(img)\n",
    "\n",
    "        # PaddleOCR 3.x API: use predict() method\n",
    "        results = ocr_engine.predict(input=img_array)\n",
    "\n",
    "        if not results:\n",
    "            return lines\n",
    "\n",
    "        # results is a list of Result objects (one per input image)\n",
    "        for res in results:\n",
    "            # Get the JSON representation of results\n",
    "            # Structure: res.json = {'res': {'rec_texts': [...], 'rec_scores': [...], 'rec_boxes': [...]}}\n",
    "            res_json = res.json if hasattr(res, \"json\") else {}\n",
    "            res_dict = res_json.get(\"res\", {}) if isinstance(res_json, dict) else {}\n",
    "\n",
    "            # Extract recognized texts, scores, and boxes\n",
    "            rec_texts = res_dict.get(\"rec_texts\", [])\n",
    "            rec_scores = res_dict.get(\"rec_scores\", [])\n",
    "            rec_boxes = res_dict.get(\"rec_boxes\", [])\n",
    "\n",
    "            # Build OcrLine objects\n",
    "            for i, text in enumerate(rec_texts):\n",
    "                conf = rec_scores[i] if i < len(rec_scores) else 0.0\n",
    "                # rec_boxes is list of [x_min, y_min, x_max, y_max]\n",
    "                if i < len(rec_boxes):\n",
    "                    box = rec_boxes[i]\n",
    "                    if isinstance(box, np.ndarray):\n",
    "                        box = box.tolist()\n",
    "                    bbox = [float(x) for x in box[:4]] if len(box) >= 4 else [0.0, 0.0, 0.0, 0.0]\n",
    "                else:\n",
    "                    bbox = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "                lines.append(OcrLine(text=str(text), conf=float(conf), bbox=bbox))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error but don't crash the whole pipeline\n",
    "        print(f\"  [WARN] OCR failed for {image_path}: {e}\", file=sys.stderr)\n",
    "        return []\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def _draw_qc(image_path: Path, lines: List[OcrLine], qc_path: Path) -> None:\n",
    "    \"\"\"Draw OCR boxes and text on image for QC visualization.\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            for ln in lines:\n",
    "                x1, y1, x2, y2 = ln.bbox\n",
    "                draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "                label = (\n",
    "                    f\"{ln.text[:30]}... ({ln.conf:.2f})\" if len(ln.text) > 30 else f\"{ln.text} ({ln.conf:.2f})\"\n",
    "                )\n",
    "                draw.text((x1, max(0, y1 - 12)), label, fill=\"red\")\n",
    "            qc_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            img.save(qc_path)\n",
    "    except Exception:\n",
    "        # QC failures should not crash main pipeline\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_ocr_step2(\n",
    "    manifest_path: Path,\n",
    "    output_path: Path,\n",
    "    qc: bool = True,\n",
    "    limit: Optional[int] = None,\n",
    "    progress_interval: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"Run OCR on all images in the manifest and write results to output_path.\"\"\"\n",
    "    if not manifest_path.exists():\n",
    "        raise FileNotFoundError(f\"Manifest not found: {manifest_path}\")\n",
    "\n",
    "    QC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Loading PaddleOCR model...\")\n",
    "    ocr_engine = _load_paddleocr()\n",
    "    print(\"PaddleOCR model loaded.\")\n",
    "\n",
    "    # Count total for progress\n",
    "    print(\"Counting images in manifest...\")\n",
    "    total_images = _count_manifest_images(manifest_path)\n",
    "    if limit:\n",
    "        total_images = min(total_images, limit)\n",
    "    print(f\"Will process up to {total_images} images.\")\n",
    "\n",
    "    processed = 0\n",
    "    no_text = 0\n",
    "    conf_sum = 0.0\n",
    "    conf_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as out_f:\n",
    "        for record in _iter_manifest_ocr(manifest_path):\n",
    "            image_path = record.get(\"image_path\")\n",
    "            if not image_path:\n",
    "                continue\n",
    "\n",
    "            img_path = Path(image_path)\n",
    "            if not img_path.exists():\n",
    "                continue\n",
    "\n",
    "            processed += 1\n",
    "            if limit and processed > limit:\n",
    "                break\n",
    "\n",
    "            # Run OCR\n",
    "            lines = _run_ocr_on_image(ocr_engine, img_path)\n",
    "\n",
    "            if not lines:\n",
    "                no_text += 1\n",
    "\n",
    "            for ln in lines:\n",
    "                conf_sum += ln.conf\n",
    "                conf_count += 1\n",
    "\n",
    "            # Write result\n",
    "            ocr_payload = {\n",
    "                \"id\": record.get(\"id\"),\n",
    "                \"dataset\": record.get(\"dataset\"),\n",
    "                \"split\": record.get(\"split\"),\n",
    "                \"image_path\": image_path,\n",
    "                \"ocr\": {\n",
    "                    \"lines\": [asdict(ln) for ln in lines]\n",
    "                },\n",
    "            }\n",
    "            out_f.write(json.dumps(ocr_payload, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            # QC image\n",
    "            if qc and lines:\n",
    "                qc_path = QC_DIR / f\"{record.get('id')}.png\"\n",
    "                _draw_qc(img_path, lines, qc_path)\n",
    "\n",
    "            # Progress reporting\n",
    "            if processed % progress_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                eta = (total_images - processed) / rate if rate > 0 else 0\n",
    "                print(\n",
    "                    f\"  Processed {processed}/{total_images} images \"\n",
    "                    f\"({processed/total_images*100:.1f}%) - \"\n",
    "                    f\"{rate:.1f} img/s - ETA: {eta/60:.1f} min\"\n",
    "                )\n",
    "\n",
    "    # Final summary\n",
    "    elapsed = time.time() - start_time\n",
    "    failure_rate = float(no_text) / float(processed) if processed > 0 else 0.0\n",
    "    avg_conf = float(conf_sum) / float(conf_count) if conf_count > 0 else 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OCR SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total images processed: {processed}\")\n",
    "    print(f\"Images with no OCR text: {no_text} (failure rate: {failure_rate:.3f})\")\n",
    "    print(f\"Total text lines detected: {conf_count}\")\n",
    "    print(f\"Mean OCR confidence: {avg_conf:.3f}\")\n",
    "    print(f\"Total time: {elapsed/60:.1f} minutes ({elapsed:.0f} seconds)\")\n",
    "    print(f\"Output written to: {output_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Run OCR for this master pipeline\n",
    "ocr_output_path = FINAL_STEP2 / \"ocr.jsonl\"\n",
    "print(\"Running OCR...\")\n",
    "run_ocr_step2(\n",
    "    manifest_path=manifest_path,\n",
    "    output_path=ocr_output_path,\n",
    "    qc=True,\n",
    "    limit=None,\n",
    "    progress_interval=200,\n",
    ")\n",
    "\n",
    "print(\"OCR output:\", ocr_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022fc69",
   "metadata": {},
   "source": [
    "## Step 3 — Pack Manifest + OCR into WebDataset Shards (Final/Step_3)\n",
    "\n",
    "Pack the unified manifest + OCR JSONL into WebDataset shards under\n",
    "`Final/Step_3/shards/`. Also writes:\n",
    "\n",
    "- `Final/Step_3/label_taxonomy.json`\n",
    "- `Final/Step_3/splits.json`\n",
    "- `Final/Step_3/stats.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Pack Manifest + OCR into WebDataset Shards (inlined from Step_3/pack_examples.py)\n",
    "\n",
    "import json\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import webdataset as wds\n",
    "\n",
    "\n",
    "def _iter_manifest_pack(manifest_path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    \"\"\"Yield records from a JSONL manifest file.\"\"\"\n",
    "    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def _load_ocr_index(ocr_path: Path) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Load OCR results into an index keyed by example id.\n",
    "\n",
    "    Each value is the `ocr.lines` list for that example.\n",
    "    \"\"\"\n",
    "    index: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    if not ocr_path.exists():\n",
    "        print(f\"[WARN] OCR file not found, continuing without OCR: {ocr_path}\")\n",
    "        return index\n",
    "\n",
    "    with ocr_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            rec_id = rec.get(\"id\")\n",
    "            ocr = rec.get(\"ocr\") or {}\n",
    "            lines = ocr.get(\"lines\") or []\n",
    "            if rec_id:\n",
    "                index[rec_id] = lines\n",
    "    return index\n",
    "\n",
    "\n",
    "def _build_text(ocr_lines: List[Dict[str, Any]], text_raw: Optional[str]) -> str:\n",
    "    \"\"\"Build combined text field from OCR lines and raw caption/text.\n",
    "\n",
    "    Pattern (for now, assuming English):\n",
    "    [OCR] <joined OCR lines> [/OCR] [CAP] <text_raw> [/CAP] <lang=en>\n",
    "    \"\"\"\n",
    "    parts: List[str] = []\n",
    "\n",
    "    texts_ocr: List[str] = []\n",
    "    for ln in ocr_lines or []:\n",
    "        t = (ln.get(\"text\") or \"\").strip()\n",
    "        try:\n",
    "            conf = float(ln.get(\"conf\", 0.0))\n",
    "        except Exception:\n",
    "            conf = 0.0\n",
    "        if not t:\n",
    "            continue\n",
    "        # Drop extremely low confidence lines\n",
    "        if conf < 0.3:\n",
    "            continue\n",
    "        texts_ocr.append(t)\n",
    "\n",
    "    if texts_ocr:\n",
    "        parts.append(\"[OCR] \" + \" \".join(texts_ocr) + \" [/OCR]\")\n",
    "\n",
    "    if text_raw is not None:\n",
    "        tr = str(text_raw).strip()\n",
    "        if tr:\n",
    "            parts.append(\"[CAP] \" + tr + \" [/CAP]\")\n",
    "\n",
    "    if parts:\n",
    "        parts.append(\"<lang=en>\")\n",
    "\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "\n",
    "def _map_labels(dataset: str, labels_raw: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Map dataset-specific `labels_raw` into a unified label schema.\"\"\"\n",
    "    result: Dict[str, Any] = {\n",
    "        \"abuse_hate\": None,\n",
    "        \"offensive\": None,\n",
    "        \"target_type\": \"unknown\",\n",
    "        \"target_group\": [],\n",
    "        \"aux\": {\n",
    "            \"dataset\": dataset,\n",
    "            \"labels_raw\": labels_raw,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Example: Hateful Memes uses 0/1 labels where 1 is hateful\n",
    "    if dataset == \"hateful_memes\":\n",
    "        try:\n",
    "            v = int(labels_raw)\n",
    "        except Exception:\n",
    "            v = labels_raw\n",
    "        if v in (0, 1):\n",
    "            result[\"abuse_hate\"] = v\n",
    "            result[\"offensive\"] = v\n",
    "            result[\"target_type\"] = \"group\" if v == 1 else \"none\"\n",
    "\n",
    "    # TODO: Extend mappings for other datasets (MAMI, MMHS150K, Memotion, HateXplain, OLID)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _ensure_default_taxonomy(taxonomy_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Ensure a default label taxonomy file exists and return its contents.\"\"\"\n",
    "    if taxonomy_path.exists():\n",
    "        with taxonomy_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    taxonomy: Dict[str, Any] = {\n",
    "        \"schema\": {\n",
    "            \"abuse_hate\": {\n",
    "                \"type\": \"binary\",\n",
    "                \"values\": [0, 1],\n",
    "                \"description\": \"Hate/abuse present (1) or not (0)\",\n",
    "            },\n",
    "            \"offensive\": {\n",
    "                \"type\": \"binary\",\n",
    "                \"values\": [0, 1],\n",
    "                \"description\": \"Offensive language present\",\n",
    "            },\n",
    "            \"target_type\": {\n",
    "                \"type\": \"categorical\",\n",
    "                \"values\": [\"none\", \"individual\", \"group\", \"other\", \"unknown\"],\n",
    "                \"description\": \"Target granularity (if any)\",\n",
    "            },\n",
    "            \"target_group\": {\n",
    "                \"type\": \"multilabel\",\n",
    "                \"description\": \"Target groups (e.g. women, black, jewish, muslim, lgbtq)\",\n",
    "            },\n",
    "            \"aux\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"Dataset-specific raw labels and metadata\",\n",
    "            },\n",
    "        },\n",
    "        \"datasets\": {\n",
    "            \"hateful_memes\": {\n",
    "                \"notes\": \"labels_raw in {0,1} mapped to abuse_hate/offensive; target_type='group' if 1 else 'none'\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    taxonomy_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with taxonomy_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(taxonomy, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return taxonomy\n",
    "\n",
    "\n",
    "def _open_shard_writers(output_dir: Path, shard_size: int) -> Dict[str, wds.ShardWriter]:\n",
    "    \"\"\"Create a dict of ShardWriter objects per split (lazily).\"\"\"\n",
    "    writers: Dict[str, wds.ShardWriter] = {}\n",
    "\n",
    "    def get_writer(split: str) -> wds.ShardWriter:\n",
    "        if split not in writers:\n",
    "            split_dir = output_dir / split\n",
    "            split_dir.mkdir(parents=True, exist_ok=True)\n",
    "            pattern = str(split_dir / \"shard-%06d.tar\")\n",
    "            writers[split] = wds.ShardWriter(pattern, maxcount=shard_size)\n",
    "        return writers[split]\n",
    "\n",
    "    # Attach the accessor to the dict for convenience\n",
    "    writers[\"__get_writer__\"] = get_writer  # type: ignore\n",
    "    return writers\n",
    "\n",
    "\n",
    "def _close_writers(writers: Dict[str, wds.ShardWriter]) -> None:\n",
    "    for key, writer in list(writers.items()):\n",
    "        if key == \"__get_writer__\":\n",
    "            continue\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def run_packing_step3(\n",
    "    manifest_path: Path,\n",
    "    ocr_path: Path,\n",
    "    output_dir: Path,\n",
    "    taxonomy_path: Path,\n",
    "    splits_path: Path,\n",
    "    examples_limit: Optional[int] = None,\n",
    "    shard_size: int = 5000,\n",
    "    image_size: int = 384,\n",
    "    progress_interval: int = 1000,\n",
    "    include_datasets: Optional[List[str]] = None,\n",
    ") -> None:\n",
    "    \"\"\"Main packing routine for the master pipeline.\"\"\"\n",
    "    if not manifest_path.exists():\n",
    "        raise FileNotFoundError(f\"Manifest not found: {manifest_path}\")\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Ensuring label taxonomy exists...\")\n",
    "    _ensure_default_taxonomy(taxonomy_path)\n",
    "\n",
    "    print(\"Loading OCR index (this may take a while for large files)...\")\n",
    "    ocr_index = _load_ocr_index(ocr_path)\n",
    "    print(f\"Loaded OCR for {len(ocr_index)} examples.\")\n",
    "\n",
    "    include_set = set(include_datasets) if include_datasets else None\n",
    "\n",
    "    writers = _open_shard_writers(output_dir, shard_size)\n",
    "    get_writer = writers[\"__get_writer__\"]  # type: ignore\n",
    "\n",
    "    # Stats\n",
    "    total_seen = 0\n",
    "    total_packed = 0\n",
    "    total_failed = 0\n",
    "    split_counts: Dict[str, int] = {}\n",
    "    split_packed: Dict[str, int] = {}\n",
    "    dataset_packed: Dict[str, int] = {}\n",
    "    splits_map: Dict[str, str] = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for record in _iter_manifest_pack(manifest_path):\n",
    "        total_seen += 1\n",
    "\n",
    "        example_id = record.get(\"id\")\n",
    "        dataset = record.get(\"dataset\", \"unknown\")\n",
    "        split = record.get(\"split\") or \"train\"\n",
    "\n",
    "        if not example_id:\n",
    "            total_failed += 1\n",
    "            continue\n",
    "\n",
    "        splits_map[example_id] = split\n",
    "        split_counts[split] = split_counts.get(split, 0) + 1\n",
    "\n",
    "        if include_set is not None and dataset not in include_set:\n",
    "            # Skip but do not treat as failure; it's a user-chosen subset\n",
    "            continue\n",
    "\n",
    "        if examples_limit is not None and total_packed >= examples_limit:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            image_path = record.get(\"image_path\")\n",
    "            text_raw = record.get(\"text_raw\")\n",
    "            labels_raw = record.get(\"labels_raw\")\n",
    "\n",
    "            # Image (optional)\n",
    "            img_bytes: Optional[bytes] = None\n",
    "            if image_path:\n",
    "                img_path = Path(image_path)\n",
    "                if img_path.exists():\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img = img.convert(\"RGB\")\n",
    "                        img = img.resize((image_size, image_size), Image.BICUBIC)\n",
    "                        buf = BytesIO()\n",
    "                        img.save(buf, format=\"PNG\")\n",
    "                        img_bytes = buf.getvalue()\n",
    "                else:\n",
    "                    # Missing image file; treat as a failure for this example\n",
    "                    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "            # OCR lines (may be empty)\n",
    "            ocr_lines = ocr_index.get(example_id, [])\n",
    "\n",
    "            # Combined text\n",
    "            combined_text = _build_text(ocr_lines, text_raw)\n",
    "\n",
    "            # Labels\n",
    "            labels = _map_labels(dataset, labels_raw)\n",
    "\n",
    "            meta = {\n",
    "                \"id\": example_id,\n",
    "                \"dataset\": dataset,\n",
    "                \"split\": split,\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "\n",
    "            sample: Dict[str, Any] = {\"__key__\": example_id}\n",
    "            if img_bytes is not None:\n",
    "                sample[\"png\"] = img_bytes\n",
    "            sample[\"txt\"] = (combined_text or \"\").encode(\"utf-8\")\n",
    "            sample[\"json\"] = json.dumps(meta, ensure_ascii=False).encode(\"utf-8\")\n",
    "\n",
    "            writer = get_writer(split)\n",
    "            writer.write(sample)\n",
    "\n",
    "            total_packed += 1\n",
    "            split_packed[split] = split_packed.get(split, 0) + 1\n",
    "            dataset_packed[dataset] = dataset_packed.get(dataset, 0) + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            total_failed += 1\n",
    "            print(f\"[WARN] failed to pack id={example_id}: {e}\", file=sys.stderr)\n",
    "\n",
    "        if progress_interval and total_seen % progress_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = total_seen / elapsed if elapsed > 0 else 0.0\n",
    "            print(\n",
    "                f\"Processed {total_seen} records, packed={total_packed}, \"\n",
    "                f\"failed={total_failed} - {rate:.1f} rec/s\"\n",
    "            )\n",
    "\n",
    "    _close_writers(writers)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    failure_rate = float(total_failed) / float(total_seen) if total_seen > 0 else 0.0\n",
    "\n",
    "    stats = {\n",
    "        \"total_manifest_records\": total_seen,\n",
    "        \"packed_examples\": total_packed,\n",
    "        \"failed_examples\": total_failed,\n",
    "        \"failure_rate\": failure_rate,\n",
    "        \"split_counts\": split_counts,\n",
    "        \"split_packed\": split_packed,\n",
    "        \"dataset_packed\": dataset_packed,\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "    }\n",
    "\n",
    "    stats_path = FINAL_STEP3 / \"stats.json\"\n",
    "    with stats_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Only create splits.json automatically if it does not exist yet\n",
    "    if not splits_path.exists():\n",
    "        with splits_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(splits_map, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 3 PACKING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total manifest records seen: {total_seen}\")\n",
    "    print(f\"Packed examples: {total_packed}\")\n",
    "    print(f\"Failed examples: {total_failed}\")\n",
    "    print(f\"Failure rate: {failure_rate:.4f}\")\n",
    "    print(f\"Elapsed time: {elapsed/60:.1f} minutes ({elapsed:.0f} seconds)\")\n",
    "    print(f\"Stats written to: {stats_path}\")\n",
    "    if not splits_path.exists():\n",
    "        print(f\"Splits mapping written to: {splits_path}\")\n",
    "    else:\n",
    "        print(f\"Splits mapping already existed at: {splits_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Run packing for this master pipeline\n",
    "packed_shards_dir = FINAL_STEP3 / \"shards\"\n",
    "packed_shards_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "label_taxonomy_path = FINAL_STEP3 / \"label_taxonomy.json\"\n",
    "splits_path = FINAL_STEP3 / \"splits.json\"\n",
    "\n",
    "print(\"Packing WebDataset shards into:\", packed_shards_dir)\n",
    "\n",
    "run_packing_step3(\n",
    "    manifest_path=manifest_path,\n",
    "    ocr_path=ocr_output_path,\n",
    "    output_dir=packed_shards_dir,\n",
    "    taxonomy_path=label_taxonomy_path,\n",
    "    splits_path=splits_path,\n",
    "    examples_limit=PACK_EXAMPLES_LIMIT,\n",
    "    shard_size=5000,\n",
    "    image_size=384,\n",
    "    progress_interval=500 if MODE == \"pilot\" else 5000,\n",
    "    include_datasets=None,\n",
    ")\n",
    "\n",
    "print(\"Shards directory:\", packed_shards_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fd004",
   "metadata": {},
   "source": [
    "## Step 4 — Train Unimodal Experts (Text & Image)\n",
    "\n",
    "This section inlines the training logic from `Step_4/step4_unimodal_experts.ipynb`,\n",
    "pointing it at the **Final/Step_3** shards. Models are still saved to\n",
    "`models/text_expert/` and `models/vision_expert/`.\n",
    "\n",
    "For the pilot mode we train **1 epoch** on the first shard; for full\n",
    "mode you can point `shard_pattern` to a larger set of shards and/or\n",
    "increase the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd547fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "models_root = project_root / \"models\"\n",
    "models_root.mkdir(exist_ok=True)\n",
    "\n",
    "# Shard pattern from Final/Step_3\n",
    "train_shards_dir = FINAL_STEP3 / \"shards\" / \"train\"\n",
    "shard_pattern = str(train_shards_dir / \"shard-000000.tar\")\n",
    "\n",
    "print(\"Training unimodal experts from shards:\", shard_pattern)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "TEXT_MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "VISION_MODEL_NAME = \"google/siglip-base-patch16-384\"\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        enc[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return enc\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]], image_processor, train: bool = True):\n",
    "        self.data = data\n",
    "        self.image_processor = image_processor\n",
    "        self.train = train\n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        if self.train:\n",
    "            img = self.aug(img)\n",
    "        inputs = self.image_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def make_text_dataset(shard_pattern: str, max_samples: Optional[int] = None):\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .to_tuple(\"txt\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, meta_obj in ds:\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "        out.append({\"text\": text, \"label\": int(y)})\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_image_dataset(shard_pattern: str, max_samples: Optional[int] = None):\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for img, meta_obj in ds:\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "        out.append({\"image\": img, \"label\": int(y)})\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "text_examples = make_text_dataset(shard_pattern, max_samples=None if MODE == \"full\" else 1000)\n",
    "image_examples = make_image_dataset(shard_pattern, max_samples=None if MODE == \"full\" else 1000)\n",
    "print(f\"Loaded {len(text_examples)} text examples and {len(image_examples)} image examples.\")\n",
    "\n",
    "# --- Train text expert ---\n",
    "\n",
    "tokenizer_text = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "model_text = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEXT_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "model_text.to(device)\n",
    "\n",
    "dataset_text = TextDataset(text_examples, tokenizer_text)\n",
    "loader_text = DataLoader(dataset_text, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer_text = torch.optim.AdamW(model_text.parameters(), lr=2e-5)\n",
    "criterion_text = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_TEXT = 1 if MODE == \"pilot\" else 2\n",
    "\n",
    "model_text.train()\n",
    "for epoch in range(EPOCHS_TEXT):\n",
    "    total_loss = 0.0\n",
    "    for batch in loader_text:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model_text(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion_text(logits, labels)\n",
    "        optimizer_text.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_text.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Text] Epoch {epoch + 1}/{EPOCHS_TEXT} - loss: {total_loss / len(loader_text):.4f}\")\n",
    "\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "model_text.save_pretrained(text_expert_dir)\n",
    "tokenizer_text.save_pretrained(text_expert_dir)\n",
    "print(\"Saved text expert to\", text_expert_dir)\n",
    "\n",
    "# --- Train vision expert ---\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "model_vision = AutoModelForImageClassification.from_pretrained(\n",
    "    VISION_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "model_vision.to(device)\n",
    "\n",
    "dataset_img = ImageDataset(image_examples, image_processor, train=True)\n",
    "loader_img = DataLoader(dataset_img, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer_v = torch.optim.AdamW(model_vision.parameters(), lr=1e-4)\n",
    "criterion_v = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_V = 1 if MODE == \"pilot\" else 2\n",
    "\n",
    "model_vision.train()\n",
    "for epoch in range(EPOCHS_V):\n",
    "    total_loss = 0.0\n",
    "    for batch in loader_img:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model_vision(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion_v(logits, labels)\n",
    "        optimizer_v.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_v.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Vision] Epoch {epoch + 1}/{EPOCHS_V} - loss: {total_loss / len(loader_img):.4f}\")\n",
    "\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "model_vision.save_pretrained(vision_expert_dir)\n",
    "image_processor.save_pretrained(vision_expert_dir)\n",
    "print(\"Saved vision expert to\", vision_expert_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15615a",
   "metadata": {},
   "source": [
    "## Step 5 — Train Multimodal Fusion Model\n",
    "\n",
    "This section mirrors `Step_5/step5_fusion.ipynb` but uses shards under\n",
    "`Final/Step_3/shards/` and the unimodal experts saved to `models/`.\n",
    "\n",
    "The fusion model weights are saved to `models/mm_fusion/fusion_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af823b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class FusionDataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict[str, Any]],\n",
    "        text_tokenizer,\n",
    "        image_processor,\n",
    "        max_length: int = 256,\n",
    "        train: bool = True,\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        self.train = train\n",
    "        self.aug = T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        if self.train:\n",
    "            img = self.aug(img)\n",
    "\n",
    "        text_enc = self.text_tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_enc = {k: v.squeeze(0) for k, v in text_enc.items()}\n",
    "\n",
    "        img_enc = self.image_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = img_enc[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_enc[\"input_ids\"],\n",
    "            \"attention_mask\": text_enc[\"attention_mask\"],\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def make_fusion_examples(shard_pattern: str, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"txt\", \"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, img, meta_obj in ds:\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "\n",
    "        out.append({\"text\": text, \"image\": img, \"label\": int(y)})\n",
    "\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "fusion_shard_pattern = shard_pattern  # reuse train shard from Final/Step_3\n",
    "fusion_examples = make_fusion_examples(\n",
    "    fusion_shard_pattern,\n",
    "    max_samples=None if MODE == \"full\" else 1000,\n",
    ")\n",
    "print(f\"Loaded {len(fusion_examples)} multimodal examples for fusion training.\")\n",
    "\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "\n",
    "text_tokenizer_fusion = AutoTokenizer.from_pretrained(text_expert_dir)\n",
    "image_processor_fusion = AutoImageProcessor.from_pretrained(vision_expert_dir)\n",
    "\n",
    "fusion_dataset = FusionDataset(\n",
    "    fusion_examples,\n",
    "    text_tokenizer=text_tokenizer_fusion,\n",
    "    image_processor=image_processor_fusion,\n",
    "    max_length=256,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "loader_fusion = DataLoader(fusion_dataset, batch_size=8, shuffle=True)\n",
    "print(\"Fusion batches per epoch:\", len(loader_fusion))\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: nn.Module,\n",
    "        vision_encoder: nn.Module,\n",
    "        t_dim: int,\n",
    "        v_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim + v_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            text_out = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            if hasattr(text_out, \"pooler_output\") and text_out.pooler_output is not None:\n",
    "                t_repr = text_out.pooler_output\n",
    "            else:\n",
    "                t_repr = text_out.last_hidden_state[:, 0, :]\n",
    "\n",
    "            vision_out = self.vision_encoder(pixel_values=pixel_values)\n",
    "            v_repr = vision_out.logits\n",
    "\n",
    "        h = torch.cat([t_repr, v_repr], dim=-1)\n",
    "        logits = self.mlp(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "text_encoder_fusion = AutoModel.from_pretrained(text_expert_dir)\n",
    "vision_encoder_fusion = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "\n",
    "text_encoder_fusion.to(device)\n",
    "vision_encoder_fusion.to(device)\n",
    "\n",
    "for p in text_encoder_fusion.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in vision_encoder_fusion.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "t_dim = text_encoder_fusion.config.hidden_size\n",
    "v_dim = vision_encoder_fusion.config.num_labels\n",
    "\n",
    "fusion_hidden = 512\n",
    "num_labels = 2\n",
    "\n",
    "fusion_model = FusionModel(\n",
    "    text_encoder=text_encoder_fusion,\n",
    "    vision_encoder=vision_encoder_fusion,\n",
    "    t_dim=t_dim,\n",
    "    v_dim=v_dim,\n",
    "    hidden_dim=fusion_hidden,\n",
    "    num_labels=num_labels,\n",
    ").to(device)\n",
    "\n",
    "optimizer_fusion = torch.optim.AdamW(fusion_model.parameters(), lr=1e-4)\n",
    "criterion_fusion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_FUSION = 1 if MODE == \"pilot\" else 2\n",
    "\n",
    "for epoch in range(EPOCHS_FUSION):\n",
    "    fusion_model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader_fusion:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "\n",
    "        logits = fusion_model(**batch)\n",
    "        loss = criterion_fusion(logits, labels)\n",
    "\n",
    "        optimizer_fusion.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_fusion.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader_fusion))\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Fusion] Epoch {epoch + 1}/{EPOCHS_FUSION} - loss: {avg_loss:.4f} - acc: {acc:.3f}\")\n",
    "\n",
    "mm_dir = models_root / \"mm_fusion\"\n",
    "mm_dir.mkdir(exist_ok=True)\n",
    "mm_fusion_path = mm_dir / \"fusion_model.pt\"\n",
    "torch.save(fusion_model.state_dict(), mm_fusion_path)\n",
    "print(\"Saved fusion model weights to\", mm_fusion_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25a507",
   "metadata": {},
   "source": [
    "## Step 6 — Evaluation (metrics only, results under Final/Step_6)\n",
    "\n",
    "We evaluate:\n",
    "\n",
    "- Text expert (`models/text_expert/`)\n",
    "- Vision expert (`models/vision_expert/`)\n",
    "- Fusion model (`models/mm_fusion/fusion_model.pt`)\n",
    "\n",
    "on the packed shard from `Final/Step_3/shards/train/shard-000000.tar` and\n",
    "save metrics to `Final/Step_6/results_{MODE}.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "\n",
    "def compute_accuracy(preds: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float((preds == labels).mean()) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int = 2) -> float:\n",
    "    f1s: List[float] = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.logical_and(preds == c, labels == c).sum()\n",
    "        fp = np.logical_and(preds == c, labels != c).sum()\n",
    "        fn = np.logical_and(preds != c, labels == c).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s)) if f1s else 0.0\n",
    "\n",
    "\n",
    "def compute_brier_score(probs_pos: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float(np.mean((probs_pos - labels) ** 2)) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_ece(probs_pos: np.ndarray, labels: np.ndarray, num_bins: int = 10) -> float:\n",
    "    bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "    ece = 0.0\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        mask = (probs_pos >= bins[i]) & (probs_pos < bins[i + 1])\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        bin_conf = probs_pos[mask].mean()\n",
    "        bin_acc = (labels[mask] == (probs_pos[mask] >= 0.5)).mean()\n",
    "        ece += (mask.sum() / n) * abs(bin_conf - bin_acc)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def summarize_metrics(logits: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "\n",
    "    acc = compute_accuracy(preds, labels_np)\n",
    "    macro_f1 = compute_macro_f1(preds, labels_np, num_classes=2)\n",
    "    brier = compute_brier_score(probs_pos, labels_np)\n",
    "    ece = compute_ece(probs_pos, labels_np, num_bins=10)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"brier\": brier,\n",
    "        \"ece\": ece,\n",
    "    }\n",
    "\n",
    "\n",
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples: List[Dict[str, Any]]):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "def collate_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    return {\"texts\": texts, \"images\": images, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Reuse fusion_examples format to build eval set from the same shard\n",
    "\n",
    "eval_examples = fusion_examples\n",
    "\n",
    "loader_eval = TorchDataLoader(\n",
    "    EvalDataset(eval_examples),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "print(\"Eval batches:\", len(loader_eval))\n",
    "\n",
    "\n",
    "def evaluate_text_expert_eval(loader: TorchDataLoader) -> Dict[str, float]:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return summarize_metrics(logits_cat, labels_cat)\n",
    "\n",
    "\n",
    "def evaluate_vision_expert_eval(loader: TorchDataLoader) -> Dict[str, float]:\n",
    "    model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc[\"pixel_values\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return summarize_metrics(logits_cat, labels_cat)\n",
    "\n",
    "\n",
    "def evaluate_fusion_eval(loader: TorchDataLoader) -> Dict[str, float]:\n",
    "    model = fusion_model  # already trained\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc_text = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "            enc_img = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=enc_text[\"input_ids\"],\n",
    "                attention_mask=enc_text[\"attention_mask\"],\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return summarize_metrics(logits_cat, labels_cat)\n",
    "\n",
    "\n",
    "results_eval: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "print(\"Evaluating text expert...\")\n",
    "results_eval[\"text_expert\"] = evaluate_text_expert_eval(loader_eval)\n",
    "print(\"Text expert:\", results_eval[\"text_expert\"])\n",
    "\n",
    "print(\"\\nEvaluating vision expert...\")\n",
    "results_eval[\"vision_expert\"] = evaluate_vision_expert_eval(loader_eval)\n",
    "print(\"Vision expert:\", results_eval[\"vision_expert\"])\n",
    "\n",
    "print(\"\\nEvaluating fusion model...\")\n",
    "results_eval[\"mm_fusion\"] = evaluate_fusion_eval(loader_eval)\n",
    "print(\"Fusion model:\", results_eval[\"mm_fusion\"])\n",
    "\n",
    "FINAL_STEP6.mkdir(parents=True, exist_ok=True)\n",
    "results_path = FINAL_STEP6 / f\"results_{MODE}.json\"\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_eval, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved evaluation metrics to\", results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c5007",
   "metadata": {},
   "source": [
    "## Step 7 — Calibration, Thresholds, and Error Analysis (Final/Step_7)\n",
    "\n",
    "This section mirrors `Step_7/step7_analysis.ipynb` but writes all\n",
    "artifacts under `Final/Step_7/`:\n",
    "\n",
    "- `calibration_{MODE}.json`\n",
    "- `thresholds_{MODE}.json`\n",
    "- `thresholds_curve_*.csv`\n",
    "- `errors_{MODE}.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # Required for threshold curve and error CSV writing\n",
    "\n",
    "FINAL_STEP7.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels_tensor = torch.tensor([ex[\"label\"] for ex in eval_examples], dtype=torch.long)\n",
    "\n",
    "# Reuse logits from evaluation by recomputing them once per model\n",
    "\n",
    "\n",
    "def collect_logits_text() -> torch.Tensor:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_eval:\n",
    "            texts = batch[\"texts\"]\n",
    "            enc = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            outputs = model(**enc)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "\n",
    "    return torch.cat(all_logits, dim=0)\n",
    "\n",
    "\n",
    "def collect_logits_vision() -> torch.Tensor:\n",
    "    model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_eval:\n",
    "            images = batch[\"images\"]\n",
    "            enc = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc[\"pixel_values\"].to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "\n",
    "    return torch.cat(all_logits, dim=0)\n",
    "\n",
    "\n",
    "def collect_logits_fusion() -> torch.Tensor:\n",
    "    model = fusion_model\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_eval:\n",
    "            texts = batch[\"texts\"]\n",
    "            images = batch[\"images\"]\n",
    "\n",
    "            enc_text = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "            enc_img = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=enc_text[\"input_ids\"],\n",
    "                attention_mask=enc_text[\"attention_mask\"],\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "            all_logits.append(logits.cpu())\n",
    "\n",
    "    return torch.cat(all_logits, dim=0)\n",
    "\n",
    "\n",
    "logits_text = collect_logits_text()\n",
    "logits_vision = collect_logits_vision()\n",
    "logits_fusion = collect_logits_fusion()\n",
    "\n",
    "print(\"Logits shapes:\", logits_text.shape, logits_vision.shape, logits_fusion.shape)\n",
    "\n",
    "\n",
    "def fit_temperature(logits: torch.Tensor, labels: torch.Tensor, max_iter: int = 200, lr: float = 0.01) -> float:\n",
    "    logits = logits.clone().to(torch.float32)\n",
    "    labels = labels.clone().to(torch.long)\n",
    "\n",
    "    T = nn.Parameter(torch.ones(1))\n",
    "    optimizer = torch.optim.Adam([T], lr=lr)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        scaled_logits = logits / T\n",
    "        loss = nn.functional.cross_entropy(scaled_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return float(T.detach().item())\n",
    "\n",
    "\n",
    "metrics_before: Dict[str, Dict[str, float]] = {}\n",
    "metrics_after: Dict[str, Dict[str, float]] = {}\n",
    "temperatures: Dict[str, float] = {}\n",
    "\n",
    "# Text expert\n",
    "metrics_before[\"text_expert\"] = summarize_metrics(logits_text, labels_tensor)\n",
    "T_text = fit_temperature(logits_text, labels_tensor)\n",
    "logits_text_cal = logits_text / T_text\n",
    "metrics_after[\"text_expert\"] = summarize_metrics(logits_text_cal, labels_tensor)\n",
    "temperatures[\"text_expert\"] = T_text\n",
    "\n",
    "# Vision expert\n",
    "metrics_before[\"vision_expert\"] = summarize_metrics(logits_vision, labels_tensor)\n",
    "T_vision = fit_temperature(logits_vision, labels_tensor)\n",
    "logits_vision_cal = logits_vision / T_vision\n",
    "metrics_after[\"vision_expert\"] = summarize_metrics(logits_vision_cal, labels_tensor)\n",
    "temperatures[\"vision_expert\"] = T_vision\n",
    "\n",
    "# Fusion model\n",
    "metrics_before[\"mm_fusion\"] = summarize_metrics(logits_fusion, labels_tensor)\n",
    "T_fusion = fit_temperature(logits_fusion, labels_tensor)\n",
    "logits_fusion_cal = logits_fusion / T_fusion\n",
    "metrics_after[\"mm_fusion\"] = summarize_metrics(logits_fusion_cal, labels_tensor)\n",
    "temperatures[\"mm_fusion\"] = T_fusion\n",
    "\n",
    "print(\"Pre-calibration metrics:\")\n",
    "for name, m in metrics_before.items():\n",
    "    print(name, m)\n",
    "\n",
    "print(\"\\nPost-calibration metrics:\")\n",
    "for name, m in metrics_after.items():\n",
    "    print(name, m)\n",
    "\n",
    "calib_path = FINAL_STEP7 / f\"calibration_{MODE}.json\"\n",
    "with calib_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(temperatures, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved calibration temperatures to\", calib_path)\n",
    "\n",
    "\n",
    "def precision_recall_f1_at_threshold(probs_pos: np.ndarray, labels: np.ndarray, threshold: float):\n",
    "    preds = (probs_pos >= threshold).astype(int)\n",
    "    tp = np.logical_and(preds == 1, labels == 1).sum()\n",
    "    fp = np.logical_and(preds == 1, labels == 0).sum()\n",
    "    fn = np.logical_and(preds == 0, labels == 1).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return float(precision), float(recall), float(f1)\n",
    "\n",
    "\n",
    "def threshold_sweep_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    model_name: str,\n",
    "    csv_path: Path,\n",
    "    num_thresholds: int = 17,\n",
    "):\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, num_thresholds)\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = -1.0\n",
    "\n",
    "    for thr in thresholds:\n",
    "        precision, recall, f1 = precision_recall_f1_at_threshold(probs_pos, labels_np, float(thr))\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"threshold\": float(thr),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        })\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = float(thr)\n",
    "\n",
    "    with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"model\", \"threshold\", \"precision\", \"recall\", \"f1\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "thr_text, f1_text = threshold_sweep_from_logits(\n",
    "    logits_text_cal,\n",
    "    labels_tensor,\n",
    "    \"text_expert\",\n",
    "    FINAL_STEP7 / f\"thresholds_curve_text_expert_{MODE}.csv\",\n",
    ")\n",
    "\n",
    "thr_vision, f1_vision = threshold_sweep_from_logits(\n",
    "    logits_vision_cal,\n",
    "    labels_tensor,\n",
    "    \"vision_expert\",\n",
    "    FINAL_STEP7 / f\"thresholds_curve_vision_expert_{MODE}.csv\",\n",
    ")\n",
    "\n",
    "thr_fusion, f1_fusion = threshold_sweep_from_logits(\n",
    "    logits_fusion_cal,\n",
    "    labels_tensor,\n",
    "    \"mm_fusion\",\n",
    "    FINAL_STEP7 / f\"thresholds_curve_mm_fusion_{MODE}.csv\",\n",
    ")\n",
    "\n",
    "thresholds_summary = {\n",
    "    \"text_expert\": {\"abuse_hate\": thr_text, \"best_f1\": f1_text},\n",
    "    \"vision_expert\": {\"abuse_hate\": thr_vision, \"best_f1\": f1_vision},\n",
    "    \"mm_fusion\": {\"abuse_hate\": thr_fusion, \"best_f1\": f1_fusion},\n",
    "}\n",
    "\n",
    "thr_path = FINAL_STEP7 / f\"thresholds_{MODE}.json\"\n",
    "with thr_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thresholds_summary, f, indent=2)\n",
    "\n",
    "print(\"Recommended thresholds:\")\n",
    "for name, info in thresholds_summary.items():\n",
    "    print(name, \"-> threshold =\", info[\"abuse_hate\"], \"best F1 =\", info[\"best_f1\"])\n",
    "\n",
    "print(\"\\nSaved threshold curves and summary to\", FINAL_STEP7)\n",
    "\n",
    "# Error table with fusion as primary model\n",
    "\n",
    "probs_text = torch.softmax(logits_text_cal, dim=-1).cpu().numpy()[:, 1]\n",
    "probs_vision = torch.softmax(logits_vision_cal, dim=-1).cpu().numpy()[:, 1]\n",
    "probs_fusion = torch.softmax(logits_fusion_cal, dim=-1).cpu().numpy()[:, 1]\n",
    "\n",
    "labels_np = labels_tensor.cpu().numpy()\n",
    "\n",
    "preds_text = (probs_text >= thr_text).astype(int)\n",
    "preds_vision = (probs_vision >= thr_vision).astype(int)\n",
    "preds_fusion = (probs_fusion >= thr_fusion).astype(int)\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, ex in enumerate(eval_examples):\n",
    "    label = int(labels_np[i])\n",
    "    pt = float(probs_text[i])\n",
    "    pv = float(probs_vision[i])\n",
    "    pf = float(probs_fusion[i])\n",
    "    yt = int(preds_text[i])\n",
    "    yv = int(preds_vision[i])\n",
    "    yf = int(preds_fusion[i])\n",
    "\n",
    "    if yf == 1 and label == 1:\n",
    "        err_type = \"TP\"\n",
    "    elif yf == 0 and label == 0:\n",
    "        err_type = \"TN\"\n",
    "    elif yf == 1 and label == 0:\n",
    "        err_type = \"FP\"\n",
    "    else:\n",
    "        err_type = \"FN\"\n",
    "\n",
    "    all_agree = int((yt == yv) and (yv == yf))\n",
    "    fusion_correct_both_wrong = int((yf == label) and (yt != label) and (yv != label))\n",
    "\n",
    "    rows.append({\n",
    "        \"index\": i,\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"label\": label,\n",
    "        \"text_prob\": pt,\n",
    "        \"text_pred\": yt,\n",
    "        \"vision_prob\": pv,\n",
    "        \"vision_pred\": yv,\n",
    "        \"fusion_prob\": pf,\n",
    "        \"fusion_pred\": yf,\n",
    "        \"fusion_error_type\": err_type,\n",
    "        \"all_models_agree\": all_agree,\n",
    "        \"fusion_correct_both_wrong\": fusion_correct_both_wrong,\n",
    "    })\n",
    "\n",
    "errors_path = FINAL_STEP7 / f\"errors_{MODE}.csv\"\n",
    "with errors_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"index\",\n",
    "            \"text\",\n",
    "            \"label\",\n",
    "            \"text_prob\",\n",
    "            \"text_pred\",\n",
    "            \"vision_prob\",\n",
    "            \"vision_pred\",\n",
    "            \"fusion_prob\",\n",
    "            \"fusion_pred\",\n",
    "            \"fusion_error_type\",\n",
    "            \"all_models_agree\",\n",
    "            \"fusion_correct_both_wrong\",\n",
    "        ],\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"\\nSaved error analysis to {errors_path}\")\n",
    "print(f\"Total examples: {len(rows)}\")\n",
    "print(f\"Fusion TP: {sum(1 for r in rows if r['fusion_error_type'] == 'TP')}\")\n",
    "print(f\"Fusion TN: {sum(1 for r in rows if r['fusion_error_type'] == 'TN')}\")\n",
    "print(f\"Fusion FP: {sum(1 for r in rows if r['fusion_error_type'] == 'FP')}\")\n",
    "print(f\"Fusion FN: {sum(1 for r in rows if r['fusion_error_type'] == 'FN')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a5e28",
   "metadata": {},
   "source": [
    "## Step 8 — Inference & Simple UI (uses models + Final/Step_7)\n",
    "\n",
    "Finally, we provide a thin inference wrapper and a small Gradio UI\n",
    "(similar to `Step_8/step8_inference.ipynb`).\n",
    "\n",
    "This cell **does not write new files**, but it **loads**:\n",
    "\n",
    "- `models/text_expert/`\n",
    "- `models/vision_expert/`\n",
    "- `models/mm_fusion/fusion_model.pt`\n",
    "- `Final/Step_7/calibration_{MODE}.json`\n",
    "- `Final/Step_7/thresholds_{MODE}.json`\n",
    "\n",
    "and exposes both a Python API and a simple browser UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6204752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image  # Required for image loading\n",
    "\n",
    "# Load calibration + thresholds from Final/Step_7\n",
    "calib_file = FINAL_STEP7 / f\"calibration_{MODE}.json\"\n",
    "thr_file = FINAL_STEP7 / f\"thresholds_{MODE}.json\"\n",
    "\n",
    "calibrations: Dict[str, float] = {}\n",
    "thresholds: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "if calib_file.exists():\n",
    "    with calib_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        calibrations = json.load(f)\n",
    "else:\n",
    "    print(\"[WARN] Calibration file not found, using T=1.0.\", calib_file)\n",
    "\n",
    "if thr_file.exists():\n",
    "    with thr_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        thresholds = json.load(f)\n",
    "else:\n",
    "    print(\"[WARN] Thresholds file not found, using threshold=0.5.\", thr_file)\n",
    "\n",
    "T_TEXT = float(calibrations.get(\"text_expert\", 1.0))\n",
    "T_VISION = float(calibrations.get(\"vision_expert\", 1.0))\n",
    "T_FUSION = float(calibrations.get(\"mm_fusion\", 1.0))\n",
    "\n",
    "THR_TEXT = float(thresholds.get(\"text_expert\", {}).get(\"abuse_hate\", 0.5))\n",
    "THR_VISION = float(thresholds.get(\"vision_expert\", {}).get(\"abuse_hate\", 0.5))\n",
    "THR_FUSION = float(thresholds.get(\"mm_fusion\", {}).get(\"abuse_hate\", 0.5))\n",
    "\n",
    "print(\"Loaded calibration + thresholds from Final/Step_7:\")\n",
    "print(\"T_TEXT =\", T_TEXT, \"T_VISION =\", T_VISION, \"T_FUSION =\", T_FUSION)\n",
    "print(\"THR_TEXT =\", THR_TEXT, \"THR_VISION =\", THR_VISION, \"THR_FUSION =\", THR_FUSION)\n",
    "\n",
    "\n",
    "def _load_image(image_path: Path) -> Image.Image:\n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def predict_text(text: str) -> Dict[str, Any]:\n",
    "    enc_text = text_tokenizer_fusion(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir).to(device)\n",
    "        logits = model(**enc_text).logits\n",
    "        logits = logits / T_TEXT\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_TEXT)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"text_expert\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_TEXT,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_image(image_path: Path) -> Dict[str, Any]:\n",
    "    img = _load_image(image_path)\n",
    "    enc_img = image_processor_fusion(images=[img], return_tensors=\"pt\")\n",
    "    pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = AutoModelForImageClassification.from_pretrained(vision_expert_dir).to(device)\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        logits = logits / T_VISION\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_VISION)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"vision_expert\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_VISION,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_fusion_infer(text: str, image_path: Path) -> Dict[str, Any]:\n",
    "    img = _load_image(image_path)\n",
    "\n",
    "    enc_text = text_tokenizer_fusion(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc_img = image_processor_fusion(images=[img], return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = enc_text[\"input_ids\"].to(device)\n",
    "    attention_mask = enc_text[\"attention_mask\"].to(device)\n",
    "    pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = fusion_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "        logits = logits / T_FUSION\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_FUSION)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"mm_fusion\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_FUSION,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_post_api(text: Optional[str] = None, image_path: Optional[Path] = None, strategy: str = \"auto\") -> Dict[str, Any]:\n",
    "    if text is None and image_path is None:\n",
    "        raise ValueError(\"Provide at least one of `text` or `image_path`.\")\n",
    "\n",
    "    strategy = strategy.lower()\n",
    "    if strategy == \"auto\":\n",
    "        if text is not None and image_path is not None:\n",
    "            return predict_fusion_infer(text, image_path)\n",
    "        if text is not None:\n",
    "            return predict_text(text)\n",
    "        return predict_image(image_path)\n",
    "\n",
    "    if strategy == \"text\":\n",
    "        if text is None:\n",
    "            raise ValueError(\"strategy='text' requires `text`.\")\n",
    "        return predict_text(text)\n",
    "\n",
    "    if strategy == \"image\":\n",
    "        if image_path is None:\n",
    "            raise ValueError(\"strategy='image' requires `image_path`.\")\n",
    "        return predict_image(image_path)\n",
    "\n",
    "    if strategy == \"fusion\":\n",
    "        if text is None or image_path is None:\n",
    "            raise ValueError(\"strategy='fusion' requires both `text` and `image_path`.\")\n",
    "        return predict_fusion_infer(text, image_path)\n",
    "\n",
    "    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "\n",
    "def _ui_predict(text: str, image):\n",
    "    text_in: Optional[str] = text.strip() if text and text.strip() else None\n",
    "\n",
    "    image_path: Optional[Path] = None\n",
    "    if image is not None:\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        tmp_file = tmp_dir / f\"mmui_{uuid.uuid4().hex}.png\"\n",
    "        image.save(tmp_file)\n",
    "        image_path = tmp_file\n",
    "\n",
    "    if text_in is None and image_path is None:\n",
    "        return \"Please provide text, an image, or both.\", {}\n",
    "\n",
    "    result = predict_post_api(text=text_in, image_path=image_path, strategy=\"auto\")\n",
    "\n",
    "    label = int(result.get(\"label\", 0))\n",
    "    prob_hate = float(result.get(\"prob_hate\", 0.0))\n",
    "    model_name = str(result.get(\"model\", \"unknown_model\"))\n",
    "\n",
    "    if label == 1:\n",
    "        explanation = (\n",
    "            f\"**Predicted: HATEFUL / ABUSIVE**  \\n\"\n",
    "            f\"Model `{model_name}` gives P(hate) = {prob_hate:.3f}.  \\n\"\n",
    "            \"This system currently makes a binary decision (hate/abuse vs non-hate); \"\n",
    "            \"it does not predict fine-grained types of hate.\"\n",
    "        )\n",
    "    else:\n",
    "        explanation = (\n",
    "            f\"**Predicted: NOT hateful / abusive**  \\n\"\n",
    "            f\"Model `{model_name}` gives P(hate) = {prob_hate:.3f} \"\n",
    "            f\"(so P(non-hate) ≈ {1.0 - prob_hate:.3f}).\"\n",
    "        )\n",
    "\n",
    "    return explanation, result\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        f\"\"\"## Multimodal Hate/Abuse Detection Demo ({MODE})\n",
    "\n",
    "Provide text, an image, or both. The system will automatically choose\n",
    "between text, image, or fusion models (using calibrated thresholds from\n",
    "Final/Step_7) to decide whether the content is hateful/abusive.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        text_in = gr.Textbox(\n",
    "            lines=4,\n",
    "            label=\"Post text (optional)\",\n",
    "            placeholder=\"Paste OCR+caption text or any post text here...\",\n",
    "        )\n",
    "        image_in = gr.Image(\n",
    "            type=\"pil\",\n",
    "            label=\"Image (optional)\",\n",
    "        )\n",
    "\n",
    "    run_btn = gr.Button(\"Run\")\n",
    "\n",
    "    explanation_out = gr.Markdown(label=\"Explanation\")\n",
    "    raw_out = gr.JSON(label=\"Raw model output\")\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=_ui_predict,\n",
    "        inputs=[text_in, image_in],\n",
    "        outputs=[explanation_out, raw_out],\n",
    "    )\n",
    "\n",
    "# Launch inside the notebook\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
