{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef127904",
   "metadata": {},
   "source": [
    "# Multimodal Hate/Abuse Detection — Master Pipeline (Steps 2–8)\n",
    "\n",
    "This notebook orchestrates the **entire pipeline** from Step 2 to Step 8\n",
    "for the multimodal hate/abuse detection system.\n",
    "\n",
    "It is designed to be **adaptive**:\n",
    "- You can run it on the **pilot subset** or the **full dataset**.\n",
    "- All intermediate artifacts for this notebook are written under\n",
    "  `Final/` (except **models**, which always live under `models/`).\n",
    "- Each step is clearly separated so you can re-run only parts as needed.\n",
    "\n",
    "> **Note:** This notebook *wraps* the existing scripts/notebooks\n",
    "> (`Step_2`, `Step_3`, `Step_4`, `Step_5`, `Step_6`, `Step_7`, `Step_8`).\n",
    "> If you want to debug internals of a specific step, you can still open\n",
    "> the original step notebooks/scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8e85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: paddleocr in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: paddlepaddle in /opt/anaconda3/lib/python3.12/site-packages (3.2.2)\n",
      "Requirement already satisfied: paddlex<3.4.0,>=3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.3.10)\n",
      "Requirement already satisfied: PyYAML>=6 in /opt/anaconda3/lib/python3.12/site-packages (from paddleocr) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from paddleocr) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from paddleocr) (4.14.1)\n",
      "Requirement already satisfied: aistudio-sdk>=0.3.5 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.3.8)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.2.0)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (6.9.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.36.0)\n",
      "Requirement already satisfied: modelscope>=1.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.32.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (23.2)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.3.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (10.4.0)\n",
      "Requirement already satisfied: prettytable in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.16.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.12.3)\n",
      "Requirement already satisfied: ruamel.yaml in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.18.6)\n",
      "Requirement already satisfied: ujson in /opt/anaconda3/lib/python3.12/site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.10.0)\n",
      "Requirement already satisfied: imagesize in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.4.1)\n",
      "Requirement already satisfied: opencv-contrib-python==4.10.0.84 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.10.0.84)\n",
      "Requirement already satisfied: pyclipper in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.3.0.post6)\n",
      "Requirement already satisfied: pypdfium2>=4 in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.30.0)\n",
      "Requirement already satisfied: python-bidi in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.6.7)\n",
      "Requirement already satisfied: shapely in /opt/anaconda3/lib/python3.12/site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.1.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (0.27.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (4.25.3)\n",
      "Requirement already satisfied: opt_einsum==3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (3.3)\n",
      "Requirement already satisfied: safetensors>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from paddlepaddle) (0.7.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.67.1)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.9.54)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (8.2.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from modelscope>=1.28.0->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (75.8.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from modelscope>=1.28.0->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->paddleocr) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->paddleocr) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->paddleocr) (2025.8.3)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from bce-python-sdk->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.23.0)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from bce-python-sdk->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.0.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->paddlepaddle) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->paddlepaddle) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->paddlepaddle) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prettytable->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.2.5)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/anaconda3/lib/python3.12/site-packages (from ruamel.yaml->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /opt/anaconda3/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (1.2.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.122.0)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.0.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.12.3)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages for the full pipeline (run once per environment).\n",
    "# You can skip this cell if everything is already installed.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core + training libs\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / datasets / utilities\n",
    "%pip install transformers datasets webdataset accelerate timm sentencepiece pillow\n",
    "\n",
    "# OCR stack for Step 2\n",
    "%pip install paddleocr paddlepaddle\n",
    "\n",
    "# Lightweight web UI for Step 8\n",
    "%pip install \"huggingface-hub>=0.33.5,<1.0\" gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384a91d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yashwanthreddy/Documents/GitHub/DL_Proj\n",
      "Final outputs root: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final\n",
      "MODE: pilot (OCR_LIMIT= 50 PACK_EXAMPLES_LIMIT= 100 )\n",
      "sys.path includes project root: True\n"
     ]
    }
   ],
   "source": [
    "# Global configuration and paths\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import sys\n",
    "\n",
    "# Robust project root detection:\n",
    "# Look for the Datasets/ folder which only exists at the true project root\n",
    "ROOT = Path.cwd().resolve()\n",
    "\n",
    "# Walk up until we find the actual project root (contains Datasets/ and Step_2/)\n",
    "project_root = ROOT\n",
    "for _ in range(5):  # max 5 levels up\n",
    "    if (project_root / \"Datasets\").is_dir() and (project_root / \"Step_2\").is_dir():\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "\n",
    "# Sanity check\n",
    "if not (project_root / \"Datasets\").is_dir():\n",
    "    raise RuntimeError(f\"Could not find project root with Datasets/ folder. Searched from {ROOT}\")\n",
    "\n",
    "FINAL_DIR = project_root / \"Final\"\n",
    "FINAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Per-step output roots for this master notebook\n",
    "FINAL_STEP2 = FINAL_DIR / \"Step_2\"\n",
    "FINAL_STEP3 = FINAL_DIR / \"Step_3\"\n",
    "FINAL_STEP6 = FINAL_DIR / \"Step_6\"\n",
    "FINAL_STEP7 = FINAL_DIR / \"Step_7\"\n",
    "\n",
    "for p in [FINAL_STEP2, FINAL_STEP3, FINAL_STEP6, FINAL_STEP7]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODE = \"pilot\"  # \"pilot\" (small subset) or \"full\" (all data)\n",
    "\n",
    "# Pilot limits (set to None for full runs)\n",
    "if MODE == \"pilot\":\n",
    "    OCR_LIMIT: Optional[int] = 50  # number of images for OCR\n",
    "    PACK_EXAMPLES_LIMIT: Optional[int] = 100  # number of examples to pack\n",
    "else:\n",
    "    OCR_LIMIT = None\n",
    "    PACK_EXAMPLES_LIMIT = None\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Final outputs root:\", FINAL_DIR)\n",
    "print(\"MODE:\", MODE, \"(OCR_LIMIT=\", OCR_LIMIT, \"PACK_EXAMPLES_LIMIT=\", PACK_EXAMPLES_LIMIT, \")\")\n",
    "\n",
    "# Make sure we can import Step_2/Step_3 modules from the project root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"sys.path includes project root:\", str(project_root) in sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1a453",
   "metadata": {},
   "source": [
    "## Step 2 — Build Manifest (Final/Step_2/data_manifest.jsonl)\n",
    "\n",
    "This section wraps `Step_2/build_data_manifest.py` and writes the\n",
    "manifest for this master notebook under `Final/Step_2/data_manifest.jsonl`.\n",
    "\n",
    "The manifest still reads raw datasets from `Datasets/`, but **no files\n",
    "are written into `Step_2/`** by this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9e4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building manifest at: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_2/data_manifest.jsonl\n",
      "\n",
      "First 3 lines of manifest:\n",
      "{\"id\": \"hateful_memes_train_42953\", \"dataset\": \"hateful_memes\", \"split\": \"train\", \"image_path\": \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets/hateful_memes/img/42953.png\", \"text_raw\": \"its their character not their color that matters\", \"labels_raw\": 0}\n",
      "{\"id\": \"hateful_memes_train_23058\", \"dataset\": \"hateful_memes\", \"split\": \"train\", \"image_path\": \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets/hateful_memes/img/23058.png\", \"text_raw\": \"don't be afraid to love again everyone is not like your ex\", \"labels_raw\": 0}\n",
      "{\"id\": \"hateful_memes_train_13894\", \"dataset\": \"hateful_memes\", \"split\": \"train\", \"image_path\": \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/Datasets/hateful_memes/img/13894.png\", \"text_raw\": \"putting bows on your pet\", \"labels_raw\": 0}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Import the Step 2 manifest builder\n",
    "manifest_mod = importlib.import_module(\"Step_2.build_data_manifest\")\n",
    "\n",
    "manifest_path = FINAL_STEP2 / \"data_manifest.jsonl\"\n",
    "manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Building manifest at:\", manifest_path)\n",
    "manifest_mod.build_manifest(manifest_path)\n",
    "\n",
    "# Quick sanity check: show a few lines\n",
    "print(\"\\nFirst 3 lines of manifest:\")\n",
    "with manifest_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4ea85",
   "metadata": {},
   "source": [
    "## Step 2 — Run OCR (Final/Step_2/ocr.jsonl)\n",
    "\n",
    "This wraps `Step_2/run_ocr.py` and writes OCR results to\n",
    "`Final/Step_2/ocr.jsonl`. QC images are written to\n",
    "`Final/Step_2/ocr_qc/`.\n",
    "\n",
    "Use `MODE` + `OCR_LIMIT` from the config cell to control whether this\n",
    "is a small pilot run or a full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ad92f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running OCR...\n",
      "Loading PaddleOCR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/Users/yashwanthreddy/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('en_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/Users/yashwanthreddy/.paddlex/official_models/en_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaddleOCR model loaded.\n",
      "Counting images in manifest...\n",
      "Will process up to 50 images.\n",
      "  Processed 50/50 images (100.0%) - 0.5 img/s - ETA: 0.0 min\n",
      "\n",
      "============================================================\n",
      "OCR SUMMARY\n",
      "============================================================\n",
      "Total images processed: 51\n",
      "Images with no OCR text: 0 (failure rate: 0.000)\n",
      "Total text lines detected: 166\n",
      "Mean OCR confidence: 0.928\n",
      "Total time: 1.6 minutes (93 seconds)\n",
      "Output written to: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_2/ocr.jsonl\n",
      "============================================================\n",
      "OCR output: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_2/ocr.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Import the Step 2 OCR runner and redirect QC directory into Final/Step_2\n",
    "ocr_mod = importlib.import_module(\"Step_2.run_ocr\")\n",
    "\n",
    "# Override QC_DIR so QC overlays are written under Final/Step_2\n",
    "ocr_mod.QC_DIR = FINAL_STEP2 / \"ocr_qc\"  # type: ignore[attr-defined]\n",
    "\n",
    "ocr_output_path = FINAL_STEP2 / \"ocr.jsonl\"\n",
    "\n",
    "print(\"Running OCR...\")\n",
    "ocr_mod.run_ocr(\n",
    "    manifest_path=manifest_path,\n",
    "    output_path=ocr_output_path,\n",
    "    qc=True,\n",
    "    limit=OCR_LIMIT,\n",
    "    progress_interval=50 if MODE == \"pilot\" else 500,\n",
    ")\n",
    "\n",
    "print(\"OCR output:\", ocr_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022fc69",
   "metadata": {},
   "source": [
    "## Step 3 — Pack Manifest + OCR into WebDataset Shards (Final/Step_3)\n",
    "\n",
    "This wraps `Step_3/pack_examples.py` and writes packed shards under\n",
    "`Final/Step_3/shards/`. It also writes:\n",
    "\n",
    "- `Final/Step_3/label_taxonomy.json`\n",
    "- `Final/Step_3/splits.json`\n",
    "- `Final/Step_3/stats.json`\n",
    "\n",
    "We temporarily patch `STEP3` inside `pack_examples.py` so that its\n",
    "internal stats file is also written under `Final/Step_3/` instead of\n",
    "`Step_3/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb1316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing WebDataset shards into: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_3/shards\n",
      "Ensuring label taxonomy exists...\n",
      "Loading OCR index (this may take a while for large files)...\n",
      "Loaded OCR for 50 examples.\n",
      "# writing /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_3/shards/train/shard-000000.tar 0 0.0 GB 0\n",
      "\n",
      "============================================================\n",
      "STEP 3 PACKING SUMMARY\n",
      "============================================================\n",
      "Total manifest records seen: 101\n",
      "Packed examples: 100\n",
      "Failed examples: 0\n",
      "Failure rate: 0.0000\n",
      "Elapsed time: 0.1 minutes (5 seconds)\n",
      "Stats written to: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_3/stats.json\n",
      "Splits mapping already existed at: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_3/splits.json\n",
      "============================================================\n",
      "Shards directory: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_3/shards\n"
     ]
    }
   ],
   "source": [
    "# Import the Step 3 packer and redirect its STEP3 root into Final/Step_3\n",
    "pack_mod = importlib.import_module(\"Step_3.pack_examples\")\n",
    "\n",
    "# Patch STEP3 so stats.json goes under Final/Step_3\n",
    "if hasattr(pack_mod, \"STEP3\"):\n",
    "    pack_mod.STEP3 = FINAL_STEP3  # type: ignore[attr-defined]\n",
    "\n",
    "packed_shards_dir = FINAL_STEP3 / \"shards\"\n",
    "packed_shards_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "label_taxonomy_path = FINAL_STEP3 / \"label_taxonomy.json\"\n",
    "splits_path = FINAL_STEP3 / \"splits.json\"\n",
    "\n",
    "print(\"Packing WebDataset shards into:\", packed_shards_dir)\n",
    "\n",
    "pack_mod.run_packing(\n",
    "    manifest_path=manifest_path,\n",
    "    ocr_path=ocr_output_path,\n",
    "    output_dir=packed_shards_dir,\n",
    "    taxonomy_path=label_taxonomy_path,\n",
    "    splits_path=splits_path,\n",
    "    examples_limit=PACK_EXAMPLES_LIMIT,\n",
    "    shard_size=5000,\n",
    "    image_size=384,\n",
    "    progress_interval=500 if MODE == \"pilot\" else 5000,\n",
    "    include_datasets=None,\n",
    ")\n",
    "\n",
    "print(\"Shards directory:\", packed_shards_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fd004",
   "metadata": {},
   "source": [
    "## Step 4 — Train Unimodal Experts (Text & Image)\n",
    "\n",
    "This section inlines the training logic from `Step_4/step4_unimodal_experts.ipynb`,\n",
    "pointing it at the **Final/Step_3** shards. Models are still saved to\n",
    "`models/text_expert/` and `models/vision_expert/`.\n",
    "\n",
    "For the pilot mode we train **1 epoch** on the first shard; for full\n",
    "mode you can point `shard_pattern` to a larger set of shards and/or\n",
    "increase the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd547fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training unimodal experts from shards: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_3/shards/train/shard-000000.tar\n",
      "Using device: cpu\n",
      "Loaded 100 text examples and 100 image examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text] Epoch 1/1 - loss: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved text expert to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/text_expert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SiglipForImageClassification were not initialized from the model checkpoint at google/siglip-base-patch16-384 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vision] Epoch 1/1 - loss: 1.0418\n",
      "Saved vision expert to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/vision_expert\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "models_root = project_root / \"models\"\n",
    "models_root.mkdir(exist_ok=True)\n",
    "\n",
    "# Shard pattern from Final/Step_3\n",
    "train_shards_dir = FINAL_STEP3 / \"shards\" / \"train\"\n",
    "shard_pattern = str(train_shards_dir / \"shard-000000.tar\")\n",
    "\n",
    "print(\"Training unimodal experts from shards:\", shard_pattern)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "TEXT_MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "VISION_MODEL_NAME = \"google/siglip-base-patch16-384\"\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        enc[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return enc\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]], image_processor, train: bool = True):\n",
    "        self.data = data\n",
    "        self.image_processor = image_processor\n",
    "        self.train = train\n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        if self.train:\n",
    "            img = self.aug(img)\n",
    "        inputs = self.image_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def make_text_dataset(shard_pattern: str, max_samples: Optional[int] = None):\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .to_tuple(\"txt\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, meta_obj in ds:\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "        out.append({\"text\": text, \"label\": int(y)})\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_image_dataset(shard_pattern: str, max_samples: Optional[int] = None):\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for img, meta_obj in ds:\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "        out.append({\"image\": img, \"label\": int(y)})\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "text_examples = make_text_dataset(shard_pattern, max_samples=None if MODE == \"full\" else 1000)\n",
    "image_examples = make_image_dataset(shard_pattern, max_samples=None if MODE == \"full\" else 1000)\n",
    "print(f\"Loaded {len(text_examples)} text examples and {len(image_examples)} image examples.\")\n",
    "\n",
    "# --- Train text expert ---\n",
    "\n",
    "tokenizer_text = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "model_text = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEXT_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "model_text.to(device)\n",
    "\n",
    "dataset_text = TextDataset(text_examples, tokenizer_text)\n",
    "loader_text = DataLoader(dataset_text, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer_text = torch.optim.AdamW(model_text.parameters(), lr=2e-5)\n",
    "criterion_text = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_TEXT = 1 if MODE == \"pilot\" else 2\n",
    "\n",
    "model_text.train()\n",
    "for epoch in range(EPOCHS_TEXT):\n",
    "    total_loss = 0.0\n",
    "    for batch in loader_text:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model_text(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion_text(logits, labels)\n",
    "        optimizer_text.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_text.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Text] Epoch {epoch + 1}/{EPOCHS_TEXT} - loss: {total_loss / len(loader_text):.4f}\")\n",
    "\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "model_text.save_pretrained(text_expert_dir)\n",
    "tokenizer_text.save_pretrained(text_expert_dir)\n",
    "print(\"Saved text expert to\", text_expert_dir)\n",
    "\n",
    "# --- Train vision expert ---\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(VISION_MODEL_NAME)\n",
    "model_vision = AutoModelForImageClassification.from_pretrained(\n",
    "    VISION_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "model_vision.to(device)\n",
    "\n",
    "dataset_img = ImageDataset(image_examples, image_processor, train=True)\n",
    "loader_img = DataLoader(dataset_img, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer_v = torch.optim.AdamW(model_vision.parameters(), lr=1e-4)\n",
    "criterion_v = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_V = 1 if MODE == \"pilot\" else 2\n",
    "\n",
    "model_vision.train()\n",
    "for epoch in range(EPOCHS_V):\n",
    "    total_loss = 0.0\n",
    "    for batch in loader_img:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model_vision(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion_v(logits, labels)\n",
    "        optimizer_v.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_v.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Vision] Epoch {epoch + 1}/{EPOCHS_V} - loss: {total_loss / len(loader_img):.4f}\")\n",
    "\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "model_vision.save_pretrained(vision_expert_dir)\n",
    "image_processor.save_pretrained(vision_expert_dir)\n",
    "print(\"Saved vision expert to\", vision_expert_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15615a",
   "metadata": {},
   "source": [
    "## Step 5 — Train Multimodal Fusion Model\n",
    "\n",
    "This section mirrors `Step_5/step5_fusion.ipynb` but uses shards under\n",
    "`Final/Step_3/shards/` and the unimodal experts saved to `models/`.\n",
    "\n",
    "The fusion model weights are saved to `models/mm_fusion/fusion_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af823b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 multimodal examples for fusion training.\n",
      "Fusion batches per epoch: 13\n",
      "[Fusion] Epoch 1/1 - loss: 0.5155 - acc: 0.810\n",
      "Saved fusion model weights to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/mm_fusion/fusion_model.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class FusionDataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict[str, Any]],\n",
    "        text_tokenizer,\n",
    "        image_processor,\n",
    "        max_length: int = 256,\n",
    "        train: bool = True,\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        self.train = train\n",
    "        self.aug = T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        if self.train:\n",
    "            img = self.aug(img)\n",
    "\n",
    "        text_enc = self.text_tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_enc = {k: v.squeeze(0) for k, v in text_enc.items()}\n",
    "\n",
    "        img_enc = self.image_processor(images=img, return_tensors=\"pt\")\n",
    "        pixel_values = img_enc[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_enc[\"input_ids\"],\n",
    "            \"attention_mask\": text_enc[\"attention_mask\"],\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def make_fusion_examples(shard_pattern: str, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"txt\", \"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, img, meta_obj in ds:\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "\n",
    "        out.append({\"text\": text, \"image\": img, \"label\": int(y)})\n",
    "\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "fusion_shard_pattern = shard_pattern  # reuse train shard from Final/Step_3\n",
    "fusion_examples = make_fusion_examples(\n",
    "    fusion_shard_pattern,\n",
    "    max_samples=None if MODE == \"full\" else 1000,\n",
    ")\n",
    "print(f\"Loaded {len(fusion_examples)} multimodal examples for fusion training.\")\n",
    "\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "\n",
    "text_tokenizer_fusion = AutoTokenizer.from_pretrained(text_expert_dir)\n",
    "image_processor_fusion = AutoImageProcessor.from_pretrained(vision_expert_dir)\n",
    "\n",
    "fusion_dataset = FusionDataset(\n",
    "    fusion_examples,\n",
    "    text_tokenizer=text_tokenizer_fusion,\n",
    "    image_processor=image_processor_fusion,\n",
    "    max_length=256,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "loader_fusion = DataLoader(fusion_dataset, batch_size=8, shuffle=True)\n",
    "print(\"Fusion batches per epoch:\", len(loader_fusion))\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: nn.Module,\n",
    "        vision_encoder: nn.Module,\n",
    "        t_dim: int,\n",
    "        v_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim + v_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            text_out = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            if hasattr(text_out, \"pooler_output\") and text_out.pooler_output is not None:\n",
    "                t_repr = text_out.pooler_output\n",
    "            else:\n",
    "                t_repr = text_out.last_hidden_state[:, 0, :]\n",
    "\n",
    "            vision_out = self.vision_encoder(pixel_values=pixel_values)\n",
    "            v_repr = vision_out.logits\n",
    "\n",
    "        h = torch.cat([t_repr, v_repr], dim=-1)\n",
    "        logits = self.mlp(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "text_encoder_fusion = AutoModel.from_pretrained(text_expert_dir)\n",
    "vision_encoder_fusion = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "\n",
    "text_encoder_fusion.to(device)\n",
    "vision_encoder_fusion.to(device)\n",
    "\n",
    "for p in text_encoder_fusion.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in vision_encoder_fusion.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "t_dim = text_encoder_fusion.config.hidden_size\n",
    "v_dim = vision_encoder_fusion.config.num_labels\n",
    "\n",
    "fusion_hidden = 512\n",
    "num_labels = 2\n",
    "\n",
    "fusion_model = FusionModel(\n",
    "    text_encoder=text_encoder_fusion,\n",
    "    vision_encoder=vision_encoder_fusion,\n",
    "    t_dim=t_dim,\n",
    "    v_dim=v_dim,\n",
    "    hidden_dim=fusion_hidden,\n",
    "    num_labels=num_labels,\n",
    ").to(device)\n",
    "\n",
    "optimizer_fusion = torch.optim.AdamW(fusion_model.parameters(), lr=1e-4)\n",
    "criterion_fusion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_FUSION = 1 if MODE == \"pilot\" else 2\n",
    "\n",
    "for epoch in range(EPOCHS_FUSION):\n",
    "    fusion_model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader_fusion:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "\n",
    "        logits = fusion_model(**batch)\n",
    "        loss = criterion_fusion(logits, labels)\n",
    "\n",
    "        optimizer_fusion.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_fusion.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader_fusion))\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"[Fusion] Epoch {epoch + 1}/{EPOCHS_FUSION} - loss: {avg_loss:.4f} - acc: {acc:.3f}\")\n",
    "\n",
    "mm_dir = models_root / \"mm_fusion\"\n",
    "mm_dir.mkdir(exist_ok=True)\n",
    "mm_fusion_path = mm_dir / \"fusion_model.pt\"\n",
    "torch.save(fusion_model.state_dict(), mm_fusion_path)\n",
    "print(\"Saved fusion model weights to\", mm_fusion_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25a507",
   "metadata": {},
   "source": [
    "## Step 6 — Evaluation (metrics only, results under Final/Step_6)\n",
    "\n",
    "We evaluate:\n",
    "\n",
    "- Text expert (`models/text_expert/`)\n",
    "- Vision expert (`models/vision_expert/`)\n",
    "- Fusion model (`models/mm_fusion/fusion_model.pt`)\n",
    "\n",
    "on the packed shard from `Final/Step_3/shards/train/shard-000000.tar` and\n",
    "save metrics to `Final/Step_6/results_{MODE}.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d41bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval batches: 13\n",
      "Evaluating text expert...\n",
      "Text expert: {'accuracy': 0.85, 'macro_f1': 0.45945945945945943, 'brier': 0.12722012048731096, 'ece': 0.6669068136811255}\n",
      "\n",
      "Evaluating vision expert...\n",
      "Vision expert: {'accuracy': 0.82, 'macro_f1': 0.5738636363636364, 'brier': 0.13332780932850632, 'ece': 0.5733259925246239}\n",
      "\n",
      "Evaluating fusion model...\n",
      "Fusion model: {'accuracy': 0.85, 'macro_f1': 0.45945945945945943, 'brier': 0.12999686437238636, 'ece': 0.7615154530107976}\n",
      "\n",
      "Saved evaluation metrics to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_6/results_pilot.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "\n",
    "def compute_accuracy(preds: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float((preds == labels).mean()) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int = 2) -> float:\n",
    "    f1s: List[float] = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.logical_and(preds == c, labels == c).sum()\n",
    "        fp = np.logical_and(preds == c, labels != c).sum()\n",
    "        fn = np.logical_and(preds != c, labels == c).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s)) if f1s else 0.0\n",
    "\n",
    "\n",
    "def compute_brier_score(probs_pos: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float(np.mean((probs_pos - labels) ** 2)) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_ece(probs_pos: np.ndarray, labels: np.ndarray, num_bins: int = 10) -> float:\n",
    "    bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "    ece = 0.0\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        mask = (probs_pos >= bins[i]) & (probs_pos < bins[i + 1])\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        bin_conf = probs_pos[mask].mean()\n",
    "        bin_acc = (labels[mask] == (probs_pos[mask] >= 0.5)).mean()\n",
    "        ece += (mask.sum() / n) * abs(bin_conf - bin_acc)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def summarize_metrics(logits: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "\n",
    "    acc = compute_accuracy(preds, labels_np)\n",
    "    macro_f1 = compute_macro_f1(preds, labels_np, num_classes=2)\n",
    "    brier = compute_brier_score(probs_pos, labels_np)\n",
    "    ece = compute_ece(probs_pos, labels_np, num_bins=10)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"brier\": brier,\n",
    "        \"ece\": ece,\n",
    "    }\n",
    "\n",
    "\n",
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples: List[Dict[str, Any]]):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "def collate_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    return {\"texts\": texts, \"images\": images, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Reuse fusion_examples format to build eval set from the same shard\n",
    "\n",
    "eval_examples = fusion_examples\n",
    "\n",
    "loader_eval = TorchDataLoader(\n",
    "    EvalDataset(eval_examples),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "print(\"Eval batches:\", len(loader_eval))\n",
    "\n",
    "\n",
    "def evaluate_text_expert_eval(loader: TorchDataLoader) -> Dict[str, float]:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return summarize_metrics(logits_cat, labels_cat)\n",
    "\n",
    "\n",
    "def evaluate_vision_expert_eval(loader: TorchDataLoader) -> Dict[str, float]:\n",
    "    model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc[\"pixel_values\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return summarize_metrics(logits_cat, labels_cat)\n",
    "\n",
    "\n",
    "def evaluate_fusion_eval(loader: TorchDataLoader) -> Dict[str, float]:\n",
    "    model = fusion_model  # already trained\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc_text = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "            enc_img = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=enc_text[\"input_ids\"],\n",
    "                attention_mask=enc_text[\"attention_mask\"],\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return summarize_metrics(logits_cat, labels_cat)\n",
    "\n",
    "\n",
    "results_eval: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "print(\"Evaluating text expert...\")\n",
    "results_eval[\"text_expert\"] = evaluate_text_expert_eval(loader_eval)\n",
    "print(\"Text expert:\", results_eval[\"text_expert\"])\n",
    "\n",
    "print(\"\\nEvaluating vision expert...\")\n",
    "results_eval[\"vision_expert\"] = evaluate_vision_expert_eval(loader_eval)\n",
    "print(\"Vision expert:\", results_eval[\"vision_expert\"])\n",
    "\n",
    "print(\"\\nEvaluating fusion model...\")\n",
    "results_eval[\"mm_fusion\"] = evaluate_fusion_eval(loader_eval)\n",
    "print(\"Fusion model:\", results_eval[\"mm_fusion\"])\n",
    "\n",
    "FINAL_STEP6.mkdir(parents=True, exist_ok=True)\n",
    "results_path = FINAL_STEP6 / f\"results_{MODE}.json\"\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_eval, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved evaluation metrics to\", results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c5007",
   "metadata": {},
   "source": [
    "## Step 7 — Calibration, Thresholds, and Error Analysis (Final/Step_7)\n",
    "\n",
    "This section mirrors `Step_7/step7_analysis.ipynb` but writes all\n",
    "artifacts under `Final/Step_7/`:\n",
    "\n",
    "- `calibration_{MODE}.json`\n",
    "- `thresholds_{MODE}.json`\n",
    "- `thresholds_curve_*.csv`\n",
    "- `errors_{MODE}.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee17cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shapes: torch.Size([100, 2]) torch.Size([100, 2]) torch.Size([100, 2])\n",
      "Pre-calibration metrics:\n",
      "text_expert {'accuracy': 0.85, 'macro_f1': 0.45945945945945943, 'brier': 0.12722012048731096, 'ece': 0.6669068136811255}\n",
      "vision_expert {'accuracy': 0.82, 'macro_f1': 0.5738636363636364, 'brier': 0.13332780932850632, 'ece': 0.5733259925246239}\n",
      "mm_fusion {'accuracy': 0.85, 'macro_f1': 0.45945945945945943, 'brier': 0.12999686437238636, 'ece': 0.7615154530107976}\n",
      "\n",
      "Post-calibration metrics:\n",
      "text_expert {'accuracy': 0.85, 'macro_f1': 0.45945945945945943, 'brier': 0.12615271780413803, 'ece': 0.7029532849788666}\n",
      "vision_expert {'accuracy': 0.82, 'macro_f1': 0.5738636363636364, 'brier': 0.11855185513197009, 'ece': 0.6846428109705448}\n",
      "mm_fusion {'accuracy': 0.85, 'macro_f1': 0.45945945945945943, 'brier': 0.12635051153895138, 'ece': 0.7000653457641601}\n",
      "\n",
      "Saved calibration temperatures to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_7/calibration_pilot.json\n",
      "Recommended thresholds:\n",
      "text_expert -> threshold = 0.15000000000000002 best F1 = 0.3333333333333333\n",
      "vision_expert -> threshold = 0.2 best F1 = 0.45833333333333326\n",
      "mm_fusion -> threshold = 0.2 best F1 = 0.31578947368421056\n",
      "\n",
      "Saved threshold curves and summary to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_7\n",
      "\n",
      "Saved error analysis to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Final/Step_7/errors_pilot.csv\n",
      "Total examples: 100\n",
      "Fusion TP: 3\n",
      "Fusion TN: 84\n",
      "Fusion FP: 1\n",
      "Fusion FN: 12\n"
     ]
    }
   ],
   "source": [
    "import csv  # Required for threshold curve and error CSV writing\n",
    "\n",
    "FINAL_STEP7.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels_tensor = torch.tensor([ex[\"label\"] for ex in eval_examples], dtype=torch.long)\n",
    "\n",
    "# Reuse logits from evaluation by recomputing them once per model\n",
    "\n",
    "\n",
    "def collect_logits_text() -> torch.Tensor:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_eval:\n",
    "            texts = batch[\"texts\"]\n",
    "            enc = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            outputs = model(**enc)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "\n",
    "    return torch.cat(all_logits, dim=0)\n",
    "\n",
    "\n",
    "def collect_logits_vision() -> torch.Tensor:\n",
    "    model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_eval:\n",
    "            images = batch[\"images\"]\n",
    "            enc = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc[\"pixel_values\"].to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "\n",
    "    return torch.cat(all_logits, dim=0)\n",
    "\n",
    "\n",
    "def collect_logits_fusion() -> torch.Tensor:\n",
    "    model = fusion_model\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_eval:\n",
    "            texts = batch[\"texts\"]\n",
    "            images = batch[\"images\"]\n",
    "\n",
    "            enc_text = text_tokenizer_fusion(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "            enc_img = image_processor_fusion(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=enc_text[\"input_ids\"],\n",
    "                attention_mask=enc_text[\"attention_mask\"],\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "            all_logits.append(logits.cpu())\n",
    "\n",
    "    return torch.cat(all_logits, dim=0)\n",
    "\n",
    "\n",
    "logits_text = collect_logits_text()\n",
    "logits_vision = collect_logits_vision()\n",
    "logits_fusion = collect_logits_fusion()\n",
    "\n",
    "print(\"Logits shapes:\", logits_text.shape, logits_vision.shape, logits_fusion.shape)\n",
    "\n",
    "\n",
    "def fit_temperature(logits: torch.Tensor, labels: torch.Tensor, max_iter: int = 200, lr: float = 0.01) -> float:\n",
    "    logits = logits.clone().to(torch.float32)\n",
    "    labels = labels.clone().to(torch.long)\n",
    "\n",
    "    T = nn.Parameter(torch.ones(1))\n",
    "    optimizer = torch.optim.Adam([T], lr=lr)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        scaled_logits = logits / T\n",
    "        loss = nn.functional.cross_entropy(scaled_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return float(T.detach().item())\n",
    "\n",
    "\n",
    "metrics_before: Dict[str, Dict[str, float]] = {}\n",
    "metrics_after: Dict[str, Dict[str, float]] = {}\n",
    "temperatures: Dict[str, float] = {}\n",
    "\n",
    "# Text expert\n",
    "metrics_before[\"text_expert\"] = summarize_metrics(logits_text, labels_tensor)\n",
    "T_text = fit_temperature(logits_text, labels_tensor)\n",
    "logits_text_cal = logits_text / T_text\n",
    "metrics_after[\"text_expert\"] = summarize_metrics(logits_text_cal, labels_tensor)\n",
    "temperatures[\"text_expert\"] = T_text\n",
    "\n",
    "# Vision expert\n",
    "metrics_before[\"vision_expert\"] = summarize_metrics(logits_vision, labels_tensor)\n",
    "T_vision = fit_temperature(logits_vision, labels_tensor)\n",
    "logits_vision_cal = logits_vision / T_vision\n",
    "metrics_after[\"vision_expert\"] = summarize_metrics(logits_vision_cal, labels_tensor)\n",
    "temperatures[\"vision_expert\"] = T_vision\n",
    "\n",
    "# Fusion model\n",
    "metrics_before[\"mm_fusion\"] = summarize_metrics(logits_fusion, labels_tensor)\n",
    "T_fusion = fit_temperature(logits_fusion, labels_tensor)\n",
    "logits_fusion_cal = logits_fusion / T_fusion\n",
    "metrics_after[\"mm_fusion\"] = summarize_metrics(logits_fusion_cal, labels_tensor)\n",
    "temperatures[\"mm_fusion\"] = T_fusion\n",
    "\n",
    "print(\"Pre-calibration metrics:\")\n",
    "for name, m in metrics_before.items():\n",
    "    print(name, m)\n",
    "\n",
    "print(\"\\nPost-calibration metrics:\")\n",
    "for name, m in metrics_after.items():\n",
    "    print(name, m)\n",
    "\n",
    "calib_path = FINAL_STEP7 / f\"calibration_{MODE}.json\"\n",
    "with calib_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(temperatures, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved calibration temperatures to\", calib_path)\n",
    "\n",
    "\n",
    "def precision_recall_f1_at_threshold(probs_pos: np.ndarray, labels: np.ndarray, threshold: float):\n",
    "    preds = (probs_pos >= threshold).astype(int)\n",
    "    tp = np.logical_and(preds == 1, labels == 1).sum()\n",
    "    fp = np.logical_and(preds == 1, labels == 0).sum()\n",
    "    fn = np.logical_and(preds == 0, labels == 1).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return float(precision), float(recall), float(f1)\n",
    "\n",
    "\n",
    "def threshold_sweep_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    model_name: str,\n",
    "    csv_path: Path,\n",
    "    num_thresholds: int = 17,\n",
    "):\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, num_thresholds)\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = -1.0\n",
    "\n",
    "    for thr in thresholds:\n",
    "        precision, recall, f1 = precision_recall_f1_at_threshold(probs_pos, labels_np, float(thr))\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"threshold\": float(thr),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        })\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = float(thr)\n",
    "\n",
    "    with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"model\", \"threshold\", \"precision\", \"recall\", \"f1\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "thr_text, f1_text = threshold_sweep_from_logits(\n",
    "    logits_text_cal,\n",
    "    labels_tensor,\n",
    "    \"text_expert\",\n",
    "    FINAL_STEP7 / f\"thresholds_curve_text_expert_{MODE}.csv\",\n",
    ")\n",
    "\n",
    "thr_vision, f1_vision = threshold_sweep_from_logits(\n",
    "    logits_vision_cal,\n",
    "    labels_tensor,\n",
    "    \"vision_expert\",\n",
    "    FINAL_STEP7 / f\"thresholds_curve_vision_expert_{MODE}.csv\",\n",
    ")\n",
    "\n",
    "thr_fusion, f1_fusion = threshold_sweep_from_logits(\n",
    "    logits_fusion_cal,\n",
    "    labels_tensor,\n",
    "    \"mm_fusion\",\n",
    "    FINAL_STEP7 / f\"thresholds_curve_mm_fusion_{MODE}.csv\",\n",
    ")\n",
    "\n",
    "thresholds_summary = {\n",
    "    \"text_expert\": {\"abuse_hate\": thr_text, \"best_f1\": f1_text},\n",
    "    \"vision_expert\": {\"abuse_hate\": thr_vision, \"best_f1\": f1_vision},\n",
    "    \"mm_fusion\": {\"abuse_hate\": thr_fusion, \"best_f1\": f1_fusion},\n",
    "}\n",
    "\n",
    "thr_path = FINAL_STEP7 / f\"thresholds_{MODE}.json\"\n",
    "with thr_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thresholds_summary, f, indent=2)\n",
    "\n",
    "print(\"Recommended thresholds:\")\n",
    "for name, info in thresholds_summary.items():\n",
    "    print(name, \"-> threshold =\", info[\"abuse_hate\"], \"best F1 =\", info[\"best_f1\"])\n",
    "\n",
    "print(\"\\nSaved threshold curves and summary to\", FINAL_STEP7)\n",
    "\n",
    "# Error table with fusion as primary model\n",
    "\n",
    "probs_text = torch.softmax(logits_text_cal, dim=-1).cpu().numpy()[:, 1]\n",
    "probs_vision = torch.softmax(logits_vision_cal, dim=-1).cpu().numpy()[:, 1]\n",
    "probs_fusion = torch.softmax(logits_fusion_cal, dim=-1).cpu().numpy()[:, 1]\n",
    "\n",
    "labels_np = labels_tensor.cpu().numpy()\n",
    "\n",
    "preds_text = (probs_text >= thr_text).astype(int)\n",
    "preds_vision = (probs_vision >= thr_vision).astype(int)\n",
    "preds_fusion = (probs_fusion >= thr_fusion).astype(int)\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, ex in enumerate(eval_examples):\n",
    "    label = int(labels_np[i])\n",
    "    pt = float(probs_text[i])\n",
    "    pv = float(probs_vision[i])\n",
    "    pf = float(probs_fusion[i])\n",
    "    yt = int(preds_text[i])\n",
    "    yv = int(preds_vision[i])\n",
    "    yf = int(preds_fusion[i])\n",
    "\n",
    "    if yf == 1 and label == 1:\n",
    "        err_type = \"TP\"\n",
    "    elif yf == 0 and label == 0:\n",
    "        err_type = \"TN\"\n",
    "    elif yf == 1 and label == 0:\n",
    "        err_type = \"FP\"\n",
    "    else:\n",
    "        err_type = \"FN\"\n",
    "\n",
    "    all_agree = int((yt == yv) and (yv == yf))\n",
    "    fusion_correct_both_wrong = int((yf == label) and (yt != label) and (yv != label))\n",
    "\n",
    "    rows.append({\n",
    "        \"index\": i,\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"label\": label,\n",
    "        \"text_prob\": pt,\n",
    "        \"text_pred\": yt,\n",
    "        \"vision_prob\": pv,\n",
    "        \"vision_pred\": yv,\n",
    "        \"fusion_prob\": pf,\n",
    "        \"fusion_pred\": yf,\n",
    "        \"fusion_error_type\": err_type,\n",
    "        \"all_models_agree\": all_agree,\n",
    "        \"fusion_correct_both_wrong\": fusion_correct_both_wrong,\n",
    "    })\n",
    "\n",
    "errors_path = FINAL_STEP7 / f\"errors_{MODE}.csv\"\n",
    "with errors_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"index\",\n",
    "            \"text\",\n",
    "            \"label\",\n",
    "            \"text_prob\",\n",
    "            \"text_pred\",\n",
    "            \"vision_prob\",\n",
    "            \"vision_pred\",\n",
    "            \"fusion_prob\",\n",
    "            \"fusion_pred\",\n",
    "            \"fusion_error_type\",\n",
    "            \"all_models_agree\",\n",
    "            \"fusion_correct_both_wrong\",\n",
    "        ],\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"\\nSaved error analysis to {errors_path}\")\n",
    "print(f\"Total examples: {len(rows)}\")\n",
    "print(f\"Fusion TP: {sum(1 for r in rows if r['fusion_error_type'] == 'TP')}\")\n",
    "print(f\"Fusion TN: {sum(1 for r in rows if r['fusion_error_type'] == 'TN')}\")\n",
    "print(f\"Fusion FP: {sum(1 for r in rows if r['fusion_error_type'] == 'FP')}\")\n",
    "print(f\"Fusion FN: {sum(1 for r in rows if r['fusion_error_type'] == 'FN')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a5e28",
   "metadata": {},
   "source": [
    "## Step 8 — Inference & Simple UI (uses models + Final/Step_7)\n",
    "\n",
    "Finally, we provide a thin inference wrapper and a small Gradio UI\n",
    "(similar to `Step_8/step8_inference.ipynb`).\n",
    "\n",
    "This cell **does not write new files**, but it **loads**:\n",
    "\n",
    "- `models/text_expert/`\n",
    "- `models/vision_expert/`\n",
    "- `models/mm_fusion/fusion_model.pt`\n",
    "- `Final/Step_7/calibration_{MODE}.json`\n",
    "- `Final/Step_7/thresholds_{MODE}.json`\n",
    "\n",
    "and exposes both a Python API and a simple browser UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6204752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded calibration + thresholds from Final/Step_7:\n",
      "T_TEXT = 0.8505802750587463 T_VISION = 0.5144212245941162 T_FUSION = 1.3515019416809082\n",
      "THR_TEXT = 0.15000000000000002 THR_VISION = 0.2 THR_FUSION = 0.2\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image  # Required for image loading\n",
    "\n",
    "# Load calibration + thresholds from Final/Step_7\n",
    "calib_file = FINAL_STEP7 / f\"calibration_{MODE}.json\"\n",
    "thr_file = FINAL_STEP7 / f\"thresholds_{MODE}.json\"\n",
    "\n",
    "calibrations: Dict[str, float] = {}\n",
    "thresholds: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "if calib_file.exists():\n",
    "    with calib_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        calibrations = json.load(f)\n",
    "else:\n",
    "    print(\"[WARN] Calibration file not found, using T=1.0.\", calib_file)\n",
    "\n",
    "if thr_file.exists():\n",
    "    with thr_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        thresholds = json.load(f)\n",
    "else:\n",
    "    print(\"[WARN] Thresholds file not found, using threshold=0.5.\", thr_file)\n",
    "\n",
    "T_TEXT = float(calibrations.get(\"text_expert\", 1.0))\n",
    "T_VISION = float(calibrations.get(\"vision_expert\", 1.0))\n",
    "T_FUSION = float(calibrations.get(\"mm_fusion\", 1.0))\n",
    "\n",
    "THR_TEXT = float(thresholds.get(\"text_expert\", {}).get(\"abuse_hate\", 0.5))\n",
    "THR_VISION = float(thresholds.get(\"vision_expert\", {}).get(\"abuse_hate\", 0.5))\n",
    "THR_FUSION = float(thresholds.get(\"mm_fusion\", {}).get(\"abuse_hate\", 0.5))\n",
    "\n",
    "print(\"Loaded calibration + thresholds from Final/Step_7:\")\n",
    "print(\"T_TEXT =\", T_TEXT, \"T_VISION =\", T_VISION, \"T_FUSION =\", T_FUSION)\n",
    "print(\"THR_TEXT =\", THR_TEXT, \"THR_VISION =\", THR_VISION, \"THR_FUSION =\", THR_FUSION)\n",
    "\n",
    "\n",
    "def _load_image(image_path: Path) -> Image.Image:\n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def predict_text(text: str) -> Dict[str, Any]:\n",
    "    enc_text = text_tokenizer_fusion(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir).to(device)\n",
    "        logits = model(**enc_text).logits\n",
    "        logits = logits / T_TEXT\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_TEXT)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"text_expert\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_TEXT,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_image(image_path: Path) -> Dict[str, Any]:\n",
    "    img = _load_image(image_path)\n",
    "    enc_img = image_processor_fusion(images=[img], return_tensors=\"pt\")\n",
    "    pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = AutoModelForImageClassification.from_pretrained(vision_expert_dir).to(device)\n",
    "        logits = model(pixel_values=pixel_values).logits\n",
    "        logits = logits / T_VISION\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_VISION)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"vision_expert\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_VISION,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_fusion_infer(text: str, image_path: Path) -> Dict[str, Any]:\n",
    "    img = _load_image(image_path)\n",
    "\n",
    "    enc_text = text_tokenizer_fusion(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc_img = image_processor_fusion(images=[img], return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = enc_text[\"input_ids\"].to(device)\n",
    "    attention_mask = enc_text[\"attention_mask\"].to(device)\n",
    "    pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = fusion_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "        logits = logits / T_FUSION\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_FUSION)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"mm_fusion\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_FUSION,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_post_api(text: Optional[str] = None, image_path: Optional[Path] = None, strategy: str = \"auto\") -> Dict[str, Any]:\n",
    "    if text is None and image_path is None:\n",
    "        raise ValueError(\"Provide at least one of `text` or `image_path`.\")\n",
    "\n",
    "    strategy = strategy.lower()\n",
    "    if strategy == \"auto\":\n",
    "        if text is not None and image_path is not None:\n",
    "            return predict_fusion_infer(text, image_path)\n",
    "        if text is not None:\n",
    "            return predict_text(text)\n",
    "        return predict_image(image_path)\n",
    "\n",
    "    if strategy == \"text\":\n",
    "        if text is None:\n",
    "            raise ValueError(\"strategy='text' requires `text`.\")\n",
    "        return predict_text(text)\n",
    "\n",
    "    if strategy == \"image\":\n",
    "        if image_path is None:\n",
    "            raise ValueError(\"strategy='image' requires `image_path`.\")\n",
    "        return predict_image(image_path)\n",
    "\n",
    "    if strategy == \"fusion\":\n",
    "        if text is None or image_path is None:\n",
    "            raise ValueError(\"strategy='fusion' requires both `text` and `image_path`.\")\n",
    "        return predict_fusion_infer(text, image_path)\n",
    "\n",
    "    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "\n",
    "def _ui_predict(text: str, image):\n",
    "    text_in: Optional[str] = text.strip() if text and text.strip() else None\n",
    "\n",
    "    image_path: Optional[Path] = None\n",
    "    if image is not None:\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        tmp_file = tmp_dir / f\"mmui_{uuid.uuid4().hex}.png\"\n",
    "        image.save(tmp_file)\n",
    "        image_path = tmp_file\n",
    "\n",
    "    if text_in is None and image_path is None:\n",
    "        return \"Please provide text, an image, or both.\", {}\n",
    "\n",
    "    result = predict_post_api(text=text_in, image_path=image_path, strategy=\"auto\")\n",
    "\n",
    "    label = int(result.get(\"label\", 0))\n",
    "    prob_hate = float(result.get(\"prob_hate\", 0.0))\n",
    "    model_name = str(result.get(\"model\", \"unknown_model\"))\n",
    "\n",
    "    if label == 1:\n",
    "        explanation = (\n",
    "            f\"**Predicted: HATEFUL / ABUSIVE**  \\n\"\n",
    "            f\"Model `{model_name}` gives P(hate) = {prob_hate:.3f}.  \\n\"\n",
    "            \"This system currently makes a binary decision (hate/abuse vs non-hate); \"\n",
    "            \"it does not predict fine-grained types of hate.\"\n",
    "        )\n",
    "    else:\n",
    "        explanation = (\n",
    "            f\"**Predicted: NOT hateful / abusive**  \\n\"\n",
    "            f\"Model `{model_name}` gives P(hate) = {prob_hate:.3f} \"\n",
    "            f\"(so P(non-hate) ≈ {1.0 - prob_hate:.3f}).\"\n",
    "        )\n",
    "\n",
    "    return explanation, result\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        f\"\"\"## Multimodal Hate/Abuse Detection Demo ({MODE})\n",
    "\n",
    "Provide text, an image, or both. The system will automatically choose\n",
    "between text, image, or fusion models (using calibrated thresholds from\n",
    "Final/Step_7) to decide whether the content is hateful/abusive.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        text_in = gr.Textbox(\n",
    "            lines=4,\n",
    "            label=\"Post text (optional)\",\n",
    "            placeholder=\"Paste OCR+caption text or any post text here...\",\n",
    "        )\n",
    "        image_in = gr.Image(\n",
    "            type=\"pil\",\n",
    "            label=\"Image (optional)\",\n",
    "        )\n",
    "\n",
    "    run_btn = gr.Button(\"Run\")\n",
    "\n",
    "    explanation_out = gr.Markdown(label=\"Explanation\")\n",
    "    raw_out = gr.JSON(label=\"Raw model output\")\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=_ui_predict,\n",
    "        inputs=[text_in, image_in],\n",
    "        outputs=[explanation_out, raw_out],\n",
    "    )\n",
    "\n",
    "# Launch inside the notebook\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
