{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fd3b1a",
   "metadata": {},
   "source": [
    "# Step 8: Inference and Serving (Pilot)\n",
    "\n",
    "This notebook provides a simple `predict(text, image_path)` interface\n",
    "for the **multimodal fusion model** trained in earlier steps.\n",
    "\n",
    "It uses:\n",
    "- `models/text_expert/`\n",
    "- `models/vision_expert/`\n",
    "- `models/mm_fusion/fusion_model.pt`\n",
    "- Calibration and thresholds from `Step_7/`.\n",
    "\n",
    "You can later point this notebook at full-data artifacts (e.g.\n",
    "`calibration_full.json`, `thresholds_full.json`) without changing the\n",
    "core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58fec251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /opt/anaconda3/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5) (1.2.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.122.0)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.0.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.12.3)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Step 8 (run once per environment).\n",
    "# You can skip this cell if everything is already installed.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core libraries\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / utilities\n",
    "%pip install transformers webdataset accelerate timm sentencepiece pillow\n",
    "\n",
    "# Lightweight web UI (in-notebook) + compatible huggingface-hub\n",
    "# We pin huggingface-hub to a version that is compatible with\n",
    "# transformers>=4.30,<5 and gradio.\n",
    "%pip install \"huggingface-hub>=0.33.5,<1.0\" gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1200c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yashwanthreddy/Documents/GitHub/DL_Proj\n",
      "Using device: cpu\n",
      "Fusion model path: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/mm_fusion/fusion_model.pt\n",
      "Using calibration file: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_7/calibration_pilot.json\n",
      "Using thresholds file: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_7/thresholds_pilot.json\n",
      "T_TEXT = 0.40586668252944946 T_VISION = 1.647761344909668 T_FUSION = 0.7356588840484619\n",
      "THR_TEXT = 0.2 THR_VISION = 0.15000000000000002 THR_FUSION = 0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, Union, Optional\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoModelForImageClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "# Detect project root so this works whether you start Jupyter in the repo root\n",
    "# or from inside Step_8/.\n",
    "cwd = Path.cwd().resolve()\n",
    "if (cwd / \"Step_3\").is_dir():\n",
    "    root = cwd\n",
    "else:\n",
    "    root = cwd.parent\n",
    "\n",
    "models_root = root / \"models\"\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "mm_fusion_dir = models_root / \"mm_fusion\"\n",
    "mm_fusion_path = mm_fusion_dir / \"fusion_model.pt\"\n",
    "\n",
    "step7_dir = root / \"Step_7\"\n",
    "\n",
    "# Choose which calibration/threshold files to use.\n",
    "# For now we default to the pilot artifacts produced in Step 7.\n",
    "MODE = \"pilot\"  # or \"full\" once you run Steps 3–7 on full data\n",
    "\n",
    "calib_file = step7_dir / f\"calibration_{MODE}.json\"\n",
    "thr_file = step7_dir / f\"thresholds_{MODE}.json\"\n",
    "\n",
    "# Fallback to pilot filenames if you keep the original names\n",
    "if not calib_file.exists():\n",
    "    alt = step7_dir / \"calibration_pilot.json\"\n",
    "    if alt.exists():\n",
    "        calib_file = alt\n",
    "if not thr_file.exists():\n",
    "    alt = step7_dir / \"thresholds_pilot.json\"\n",
    "    if alt.exists():\n",
    "        thr_file = alt\n",
    "\n",
    "calibrations: Dict[str, float] = {}\n",
    "thresholds: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "if calib_file.exists():\n",
    "    with calib_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        calibrations = json.load(f)\n",
    "else:\n",
    "    print(\"[WARN] Calibration file not found, using T=1.0.\", calib_file)\n",
    "\n",
    "if thr_file.exists():\n",
    "    with thr_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        thresholds = json.load(f)\n",
    "else:\n",
    "    print(\"[WARN] Thresholds file not found, using threshold=0.5.\", thr_file)\n",
    "\n",
    "# Temperature scaling parameters from Step 7\n",
    "T_TEXT = float(calibrations.get(\"text_expert\", 1.0))\n",
    "T_VISION = float(calibrations.get(\"vision_expert\", 1.0))\n",
    "T_FUSION = float(calibrations.get(\"mm_fusion\", 1.0))\n",
    "\n",
    "# Decision thresholds from Step 7\n",
    "THR_TEXT = float(thresholds.get(\"text_expert\", {}).get(\"abuse_hate\", 0.5))\n",
    "THR_VISION = float(thresholds.get(\"vision_expert\", {}).get(\"abuse_hate\", 0.5))\n",
    "THR_FUSION = float(thresholds.get(\"mm_fusion\", {}).get(\"abuse_hate\", 0.5))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Project root:\", root)\n",
    "print(\"Using device:\", device)\n",
    "print(\"Fusion model path:\", mm_fusion_path)\n",
    "print(\"Using calibration file:\", calib_file)\n",
    "print(\"Using thresholds file:\", thr_file)\n",
    "print(\"T_TEXT =\", T_TEXT, \"T_VISION =\", T_VISION, \"T_FUSION =\", T_FUSION)\n",
    "print(\"THR_TEXT =\", THR_TEXT, \"THR_VISION =\", THR_VISION, \"THR_FUSION =\", THR_FUSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04453025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion model definition (same architecture as in Steps 5–7)\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: nn.Module,\n",
    "        vision_encoder: nn.Module,\n",
    "        t_dim: int,\n",
    "        v_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim + v_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Encoders are frozen; we only use them to produce representations.\n",
    "        with torch.no_grad():\n",
    "            text_out = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            if hasattr(text_out, \"pooler_output\") and text_out.pooler_output is not None:\n",
    "                t_repr = text_out.pooler_output\n",
    "            else:\n",
    "                t_repr = text_out.last_hidden_state[:, 0, :]\n",
    "\n",
    "            vision_out = self.vision_encoder(pixel_values=pixel_values)\n",
    "            v_repr = vision_out.logits\n",
    "\n",
    "        h = torch.cat([t_repr, v_repr], dim=-1)\n",
    "        logits = self.mlp(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def load_fusion_model() -> FusionModel:\n",
    "    # Recreate encoders in the same way as training\n",
    "    text_encoder = AutoModel.from_pretrained(text_expert_dir)\n",
    "    vision_encoder = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "\n",
    "    text_encoder.to(device)\n",
    "    vision_encoder.to(device)\n",
    "\n",
    "    for p in text_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in vision_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    t_dim = text_encoder.config.hidden_size\n",
    "    v_dim = vision_encoder.config.num_labels\n",
    "\n",
    "    fusion_hidden = 512\n",
    "    num_labels = 2\n",
    "\n",
    "    model = FusionModel(\n",
    "        text_encoder=text_encoder,\n",
    "        vision_encoder=vision_encoder,\n",
    "        t_dim=t_dim,\n",
    "        v_dim=v_dim,\n",
    "        hidden_dim=fusion_hidden,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    state_dict = torch.load(mm_fusion_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8209c082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models:\n",
      " - text_expert (T_TEXT = 0.40586668252944946 , THR_TEXT = 0.2 )\n",
      " - vision_expert (T_VISION = 1.647761344909668 , THR_VISION = 0.15000000000000002 )\n",
      " - mm_fusion (T_FUSION = 0.7356588840484619 , THR_FUSION = 0.15000000000000002 )\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer, image processor, unimodal experts, and fusion model\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_expert_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_expert_dir)\n",
    "\n",
    "# Text expert classifier\n",
    "text_cls_model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "text_cls_model.to(device)\n",
    "text_cls_model.eval()\n",
    "\n",
    "# Vision expert classifier\n",
    "vision_cls_model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "vision_cls_model.to(device)\n",
    "vision_cls_model.eval()\n",
    "\n",
    "# Multimodal fusion model\n",
    "fusion_model = load_fusion_model()\n",
    "\n",
    "print(\"Loaded models:\")\n",
    "print(\" - text_expert (T_TEXT =\", T_TEXT, \", THR_TEXT =\", THR_TEXT, \")\")\n",
    "print(\" - vision_expert (T_VISION =\", T_VISION, \", THR_VISION =\", THR_VISION, \")\")\n",
    "print(\" - mm_fusion (T_FUSION =\", T_FUSION, \", THR_FUSION =\", THR_FUSION, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b33c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_image(image_path: Union[str, Path]) -> Image.Image:\n",
    "    image_path = Path(image_path)\n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def predict_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Predict using the text expert only.\"\"\"\n",
    "    enc_text = text_tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = text_cls_model(**enc_text).logits\n",
    "        logits = logits / T_TEXT\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_TEXT)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"text_expert\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_TEXT,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_image(image_path: Union[str, Path]) -> Dict[str, Any]:\n",
    "    \"\"\"Predict using the vision expert only.\"\"\"\n",
    "    img = _load_image(image_path)\n",
    "\n",
    "    enc_img = image_processor(images=[img], return_tensors=\"pt\")\n",
    "    pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = vision_cls_model(pixel_values=pixel_values).logits\n",
    "        logits = logits / T_VISION\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_VISION)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"vision_expert\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_VISION,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_fusion(text: str, image_path: Union[str, Path]) -> Dict[str, Any]:\n",
    "    \"\"\"Predict using the multimodal fusion model (text + image).\"\"\"\n",
    "    img = _load_image(image_path)\n",
    "\n",
    "    enc_text = text_tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    enc_img = image_processor(images=[img], return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = enc_text[\"input_ids\"].to(device)\n",
    "    attention_mask = enc_text[\"attention_mask\"].to(device)\n",
    "    pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = fusion_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "        # Apply temperature scaling from Step 7\n",
    "        logits = logits / T_FUSION\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    prob_hate = float(probs[1])\n",
    "    label = int(prob_hate >= THR_FUSION)\n",
    "\n",
    "    return {\n",
    "        \"model\": \"mm_fusion\",\n",
    "        \"prob_hate\": prob_hate,\n",
    "        \"threshold\": THR_FUSION,\n",
    "        \"label\": label,\n",
    "        \"probs\": probs.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_post(\n",
    "    text: Optional[str] = None,\n",
    "    image_path: Optional[Union[str, Path]] = None,\n",
    "    strategy: str = \"auto\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"High-level prediction API supporting text, image, or both.\n",
    "\n",
    "    Args:\n",
    "        text: Text string (OCR + caption or raw text).\n",
    "        image_path: Path to the meme image file.\n",
    "        strategy:\n",
    "            - \"auto\": fusion if both, else whichever modality is present.\n",
    "            - \"text\": force text expert (requires text).\n",
    "            - \"image\": force vision expert (requires image_path).\n",
    "            - \"fusion\": force fusion model (requires both).\n",
    "\n",
    "    Returns:\n",
    "        Prediction dict with probability, label, model name, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    if text is None and image_path is None:\n",
    "        raise ValueError(\"Provide at least one of `text` or `image_path`.\")\n",
    "\n",
    "    strategy = strategy.lower()\n",
    "    if strategy not in {\"auto\", \"text\", \"image\", \"fusion\"}:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    if strategy == \"auto\":\n",
    "        if text is not None and image_path is not None:\n",
    "            return predict_fusion(text, image_path)\n",
    "        if text is not None:\n",
    "            return predict_text(text)\n",
    "        return predict_image(image_path)\n",
    "\n",
    "    if strategy == \"text\":\n",
    "        if text is None:\n",
    "            raise ValueError(\"strategy='text' requires `text`.\")\n",
    "        return predict_text(text)\n",
    "\n",
    "    if strategy == \"image\":\n",
    "        if image_path is None:\n",
    "            raise ValueError(\"strategy='image' requires `image_path`.\")\n",
    "        return predict_image(image_path)\n",
    "\n",
    "    # strategy == \"fusion\"\n",
    "    if text is None or image_path is None:\n",
    "        raise ValueError(\"strategy='fusion' requires both `text` and `image_path`.\")\n",
    "    return predict_fusion(text, image_path)\n",
    "\n",
    "\n",
    "# Backwards-compatible alias: original API name\n",
    "predict = predict_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99468408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'mm_fusion', 'prob_hate': 0.08315027505159378, 'threshold': 0.15000000000000002, 'label': 0, 'probs': [0.916849672794342, 0.08315027505159378]}\n",
      "Ready to call predict(text, image_path). Set example_image_path and uncomment the call above.\n"
     ]
    }
   ],
   "source": [
    "# Example usage (adjust paths before running)\n",
    "\n",
    "# Example combined text (OCR + caption). In practice, you can reuse the\n",
    "# same text-building logic from Step 3 or Step 4, or pass raw meme text.\n",
    "example_text = \"[OCR] example ocr text [/OCR] [CAP] example caption text [/CAP] <lang=en>\"\n",
    "\n",
    "# TODO: set this to an actual meme image path from your dataset\n",
    "# For example:\n",
    "example_image_path = \"/Users/yashwanthreddy/Documents/GitHub/DL_Proj/98734.png\"\n",
    "result = predict(example_text, example_image_path)\n",
    "print(result)\n",
    "\n",
    "print(\"Ready to call predict(text, image_path). Set example_image_path and uncomment the call above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793df91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lightweight Gradio UI (final cell)\n",
    "\n",
    "\n",
    "def _ui_predict(text: str, image):\n",
    "    \"\"\"Wrapper for Gradio: takes raw text + PIL image, uses predict_post.\"\"\"\n",
    "    # Normalize inputs\n",
    "    text_in: Optional[str]\n",
    "    if text is not None and str(text).strip():\n",
    "        text_in = str(text).strip()\n",
    "    else:\n",
    "        text_in = None\n",
    "\n",
    "    image_path: Optional[str] = None\n",
    "    if image is not None:\n",
    "        # Save uploaded image to a temporary file and pass its path\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        tmp_file = tmp_dir / f\"mmui_{uuid.uuid4().hex}.png\"\n",
    "        image.save(tmp_file)\n",
    "        image_path = str(tmp_file)\n",
    "\n",
    "    if text_in is None and image_path is None:\n",
    "        return \"Please provide text, an image, or both.\", {}\n",
    "\n",
    "    result = predict_post(text=text_in, image_path=image_path, strategy=\"auto\")\n",
    "\n",
    "    label = int(result.get(\"label\", 0))\n",
    "    prob_hate = float(result.get(\"prob_hate\", 0.0))\n",
    "    model_name = str(result.get(\"model\", \"unknown_model\"))\n",
    "\n",
    "    if label == 1:\n",
    "        explanation = (\n",
    "            f\"**Predicted: HATEFUL / ABUSIVE**  \\n\"\n",
    "            f\"Model `{model_name}` gives P(hate) = {prob_hate:.3f}.  \\n\"\n",
    "            \"This system currently makes a binary decision (hate/abuse vs non-hate); \"\n",
    "            \"it does not predict fine-grained types of hate.\"\n",
    "        )\n",
    "    else:\n",
    "        explanation = (\n",
    "            f\"**Predicted: NOT hateful / abusive**  \\n\"\n",
    "            f\"Model `{model_name}` gives P(hate) = {prob_hate:.3f} \"\n",
    "            f\"(so P(non-hate) ≈ {1.0 - prob_hate:.3f}).\"\n",
    "        )\n",
    "\n",
    "    return explanation, result\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"## Multimodal Hate/Abuse Detection Demo (Pilot)\n",
    "\n",
    "Provide text, an image, or both. The system will automatically choose\n",
    "between text, image, or fusion models (using calibrated thresholds from\n",
    "Steps 6–7) to decide whether the content is hateful/abusive.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        text_in = gr.Textbox(\n",
    "            lines=4,\n",
    "            label=\"Post text (optional)\",\n",
    "            placeholder=\"Paste OCR+caption text or any post text here...\",\n",
    "        )\n",
    "        image_in = gr.Image(\n",
    "            type=\"pil\",\n",
    "            label=\"Image (optional)\",\n",
    "        )\n",
    "\n",
    "    run_btn = gr.Button(\"Run\")\n",
    "\n",
    "    explanation_out = gr.Markdown(label=\"Explanation\")\n",
    "    raw_out = gr.JSON(label=\"Raw model output\")\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=_ui_predict,\n",
    "        inputs=[text_in, image_in],\n",
    "        outputs=[explanation_out, raw_out],\n",
    "    )\n",
    "\n",
    "# Launch inside the notebook\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
