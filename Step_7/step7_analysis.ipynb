{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578bcc30",
   "metadata": {},
   "source": [
    "# Step 7: Thresholds, Calibration, and Robustness (Pilot)\n",
    "\n",
    "This notebook implements Step 7 for the 50-example pilot shard.\n",
    "\n",
    "It:\n",
    "- Re-evaluates the **text expert**, **vision expert**, and **fusion model**.\n",
    "- Fits **temperature scaling** per model for better calibration.\n",
    "- Sweeps **decision thresholds** for `abuse_hate` and picks recommendations.\n",
    "- Generates **CSV/JSON artifacts** and an **error table** under `Step_7/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97cd507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Step 7 (run once per environment).\n",
    "# You can skip this cell if everything is already installed.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core libraries\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / datasets / training utilities\n",
    "%pip install transformers datasets webdataset accelerate timm sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca3760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Shard pattern: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_3/shards/train/shard-000000.tar\n",
      "Text expert dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/text_expert\n",
      "Vision expert dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/vision_expert\n",
      "Fusion model path: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/mm_fusion/fusion_model.pt\n",
      "Step 7 output dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_7\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "\n",
    "# Detect project root so this works whether you start Jupyter in the repo root\n",
    "# or from inside Step_7/.\n",
    "cwd = Path.cwd().resolve()\n",
    "if (cwd / \"Step_3\").is_dir():\n",
    "    root = cwd\n",
    "else:\n",
    "    root = cwd.parent\n",
    "\n",
    "step3 = root / \"Step_3\"\n",
    "shards_dir = step3 / \"shards\" / \"train\"\n",
    "shard_pattern = str(shards_dir / \"shard-000000.tar\")  # 50-example pilot shard\n",
    "\n",
    "models_root = root / \"models\"\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "mm_fusion_dir = models_root / \"mm_fusion\"\n",
    "mm_fusion_path = mm_fusion_dir / \"fusion_model.pt\"\n",
    "\n",
    "step7_dir = root / \"Step_7\"\n",
    "step7_dir.mkdir(exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"Shard pattern:\", shard_pattern)\n",
    "print(\"Text expert dir:\", text_expert_dir)\n",
    "print(\"Vision expert dir:\", vision_expert_dir)\n",
    "print(\"Fusion model path:\", mm_fusion_path)\n",
    "print(\"Step 7 output dir:\", step7_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52889ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 evaluation examples for Step 7.\n"
     ]
    }
   ],
   "source": [
    "def make_eval_examples(shard_pattern: str, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create a list of {text, image, label} from WebDataset shards for analysis.\"\"\"\n",
    "\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"txt\", \"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, img, meta_obj in ds:\n",
    "        # Decode text\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        # Decode metadata\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            \"text\": text,\n",
    "            \"image\": img,\n",
    "            \"label\": int(y),\n",
    "        })\n",
    "\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "eval_examples = make_eval_examples(shard_pattern, max_samples=1000)\n",
    "print(f\"Loaded {len(eval_examples)} evaluation examples for Step 7.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc23f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 7\n"
     ]
    }
   ],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, examples: List[Dict[str, Any]]):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "def collate_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    return {\"texts\": texts, \"images\": images, \"labels\": labels}\n",
    "\n",
    "\n",
    "dataset_eval = EvalDataset(eval_examples)\n",
    "loader_eval = DataLoader(dataset_eval, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Batches per epoch:\", len(loader_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ff9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float((preds == labels).mean()) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int = 2) -> float:\n",
    "    f1s: List[float] = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.logical_and(preds == c, labels == c).sum()\n",
    "        fp = np.logical_and(preds == c, labels != c).sum()\n",
    "        fn = np.logical_and(preds != c, labels == c).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s)) if f1s else 0.0\n",
    "\n",
    "\n",
    "def compute_brier_score(probs_pos: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float(np.mean((probs_pos - labels) ** 2)) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_ece(probs_pos: np.ndarray, labels: np.ndarray, num_bins: int = 10) -> float:\n",
    "    bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "    ece = 0.0\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        mask = (probs_pos >= bins[i]) & (probs_pos < bins[i + 1])\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        bin_conf = probs_pos[mask].mean()\n",
    "        bin_acc = (labels[mask] == (probs_pos[mask] >= 0.5)).mean()\n",
    "        ece += (mask.sum() / n) * abs(bin_conf - bin_acc)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def summarize_metrics(logits: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "\n",
    "    acc = compute_accuracy(preds, labels_np)\n",
    "    macro_f1 = compute_macro_f1(preds, labels_np, num_classes=2)\n",
    "    brier = compute_brier_score(probs_pos, labels_np)\n",
    "    ece = compute_ece(probs_pos, labels_np, num_bins=10)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"brier\": brier,\n",
    "        \"ece\": ece,\n",
    "    }\n",
    "\n",
    "\n",
    "def precision_recall_f1_at_threshold(probs_pos: np.ndarray, labels: np.ndarray, threshold: float) -> Tuple[float, float, float]:\n",
    "    preds = (probs_pos >= threshold).astype(int)\n",
    "    tp = np.logical_and(preds == 1, labels == 1).sum()\n",
    "    fp = np.logical_and(preds == 1, labels == 0).sum()\n",
    "    fn = np.logical_and(preds == 0, labels == 1).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return float(precision), float(recall), float(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10687614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting logits for text expert...\n",
      "Text logits shape: torch.Size([50, 2])\n",
      "Collecting logits for vision expert...\n",
      "Vision logits shape: torch.Size([50, 2])\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers / processors and collect logits for text and vision experts\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_expert_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_expert_dir)\n",
    "\n",
    "\n",
    "def get_text_logits(loader: DataLoader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = text_tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return logits_cat, labels_cat\n",
    "\n",
    "\n",
    "def get_vision_logits(loader: DataLoader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = image_processor(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc[\"pixel_values\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return logits_cat, labels_cat\n",
    "\n",
    "\n",
    "print(\"Collecting logits for text expert...\")\n",
    "logits_text, labels_text = get_text_logits(loader_eval)\n",
    "print(\"Text logits shape:\", logits_text.shape)\n",
    "\n",
    "print(\"Collecting logits for vision expert...\")\n",
    "logits_vision, labels_vision = get_vision_logits(loader_eval)\n",
    "print(\"Vision logits shape:\", logits_vision.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7225589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting logits for fusion model...\n",
      "Fusion logits shape: torch.Size([50, 2])\n"
     ]
    }
   ],
   "source": [
    "# Fusion model definition and logits collection\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: nn.Module,\n",
    "        vision_encoder: nn.Module,\n",
    "        t_dim: int,\n",
    "        v_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim + v_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            text_out = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            if hasattr(text_out, \"pooler_output\") and text_out.pooler_output is not None:\n",
    "                t_repr = text_out.pooler_output\n",
    "            else:\n",
    "                t_repr = text_out.last_hidden_state[:, 0, :]\n",
    "\n",
    "            vision_out = self.vision_encoder(pixel_values=pixel_values)\n",
    "            v_repr = vision_out.logits\n",
    "\n",
    "        h = torch.cat([t_repr, v_repr], dim=-1)\n",
    "        logits = self.mlp(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def load_fusion_model() -> FusionModel:\n",
    "    text_encoder = AutoModel.from_pretrained(text_expert_dir)\n",
    "    vision_encoder = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "\n",
    "    text_encoder.to(device)\n",
    "    vision_encoder.to(device)\n",
    "\n",
    "    for p in text_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in vision_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    t_dim = text_encoder.config.hidden_size\n",
    "    v_dim = vision_encoder.config.num_labels\n",
    "\n",
    "    fusion_hidden = 512\n",
    "    num_labels = 2\n",
    "\n",
    "    model = FusionModel(\n",
    "        text_encoder=text_encoder,\n",
    "        vision_encoder=vision_encoder,\n",
    "        t_dim=t_dim,\n",
    "        v_dim=v_dim,\n",
    "        hidden_dim=fusion_hidden,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    state_dict = torch.load(mm_fusion_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_fusion_logits(loader: DataLoader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model = load_fusion_model()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc_text = text_tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "            enc_img = image_processor(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=enc_text[\"input_ids\"],\n",
    "                attention_mask=enc_text[\"attention_mask\"],\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    return logits_cat, labels_cat\n",
    "\n",
    "\n",
    "print(\"Collecting logits for fusion model...\")\n",
    "logits_fusion, labels_fusion = get_fusion_logits(loader_eval)\n",
    "print(\"Fusion logits shape:\", logits_fusion.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a72c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-calibration metrics:\n",
      "text_expert {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.12276667188716842, 'ece': 0.6138291072845459}\n",
      "vision_expert {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.09003111768167933, 'ece': 0.8844988291338086}\n",
      "mm_fusion {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.08550730560385912, 'ece': 0.7489281076192857}\n",
      "\n",
      "Post-calibration metrics (temperature scaling):\n",
      "text_expert {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.08715975735207579, 'ece': 0.8113224555552006}\n",
      "vision_expert {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.08018320326215748, 'ece': 0.8272676227986813}\n",
      "mm_fusion {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.08318219306679688, 'ece': 0.8204712355136872}\n",
      "\n",
      "Saved calibration temperatures to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_7/calibration_pilot.json\n"
     ]
    }
   ],
   "source": [
    "# Temperature scaling per model and pre/post-calibration metrics\n",
    "\n",
    "# Ensure labels are consistent across models\n",
    "assert torch.equal(labels_text, labels_vision)\n",
    "assert torch.equal(labels_text, labels_fusion)\n",
    "labels = labels_fusion.clone()\n",
    "\n",
    "\n",
    "def fit_temperature(logits: torch.Tensor, labels: torch.Tensor, max_iter: int = 200, lr: float = 0.01) -> float:\n",
    "    \"\"\"Fit a single temperature parameter using cross-entropy minimization.\"\"\"\n",
    "    logits = logits.clone().to(torch.float32)\n",
    "    labels = labels.clone().to(torch.long)\n",
    "\n",
    "    T = nn.Parameter(torch.ones(1))\n",
    "    optimizer = torch.optim.Adam([T], lr=lr)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        scaled_logits = logits / T\n",
    "        loss = F.cross_entropy(scaled_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return float(T.detach().item())\n",
    "\n",
    "\n",
    "metrics_before: Dict[str, Dict[str, float]] = {}\n",
    "metrics_after: Dict[str, Dict[str, float]] = {}\n",
    "temperatures: Dict[str, float] = {}\n",
    "\n",
    "# Text expert\n",
    "metrics_before[\"text_expert\"] = summarize_metrics(logits_text, labels)\n",
    "T_text = fit_temperature(logits_text, labels)\n",
    "logits_text_cal = logits_text / T_text\n",
    "metrics_after[\"text_expert\"] = summarize_metrics(logits_text_cal, labels)\n",
    "temperatures[\"text_expert\"] = T_text\n",
    "\n",
    "# Vision expert\n",
    "metrics_before[\"vision_expert\"] = summarize_metrics(logits_vision, labels)\n",
    "T_vision = fit_temperature(logits_vision, labels)\n",
    "logits_vision_cal = logits_vision / T_vision\n",
    "metrics_after[\"vision_expert\"] = summarize_metrics(logits_vision_cal, labels)\n",
    "temperatures[\"vision_expert\"] = T_vision\n",
    "\n",
    "# Fusion model\n",
    "metrics_before[\"mm_fusion\"] = summarize_metrics(logits_fusion, labels)\n",
    "T_fusion = fit_temperature(logits_fusion, labels)\n",
    "logits_fusion_cal = logits_fusion / T_fusion\n",
    "metrics_after[\"mm_fusion\"] = summarize_metrics(logits_fusion_cal, labels)\n",
    "temperatures[\"mm_fusion\"] = T_fusion\n",
    "\n",
    "print(\"Pre-calibration metrics:\")\n",
    "for name, m in metrics_before.items():\n",
    "    print(name, m)\n",
    "\n",
    "print(\"\\nPost-calibration metrics (temperature scaling):\")\n",
    "for name, m in metrics_after.items():\n",
    "    print(name, m)\n",
    "\n",
    "calib_path = step7_dir / \"calibration_pilot.json\"\n",
    "with calib_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(temperatures, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved calibration temperatures to\", calib_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b20f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended thresholds (pilot):\n",
      "text_expert -> threshold = 0.2 best F1 = 0.33333333333333337\n",
      "vision_expert -> threshold = 0.15000000000000002 best F1 = 0.5714285714285715\n",
      "mm_fusion -> threshold = 0.15000000000000002 best F1 = 0.4444444444444445\n",
      "\n",
      "Saved threshold curves and summary to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_7\n"
     ]
    }
   ],
   "source": [
    "# Threshold sweeps for each model (using calibrated logits)\n",
    "\n",
    "\n",
    "def threshold_sweep_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    model_name: str,\n",
    "    csv_path: Path,\n",
    "    num_thresholds: int = 17,\n",
    ") -> Tuple[float, float]:\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    thresholds = np.linspace(0.1, 0.9, num_thresholds)\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = -1.0\n",
    "\n",
    "    for thr in thresholds:\n",
    "        precision, recall, f1 = precision_recall_f1_at_threshold(probs_pos, labels_np, float(thr))\n",
    "        rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"threshold\": float(thr),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        })\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = float(thr)\n",
    "\n",
    "    with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"model\", \"threshold\", \"precision\", \"recall\", \"f1\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "thr_text, f1_text = threshold_sweep_from_logits(\n",
    "    logits_text_cal,\n",
    "    labels,\n",
    "    \"text_expert\",\n",
    "    step7_dir / \"thresholds_curve_text_expert_pilot.csv\",\n",
    ")\n",
    "\n",
    "thr_vision, f1_vision = threshold_sweep_from_logits(\n",
    "    logits_vision_cal,\n",
    "    labels,\n",
    "    \"vision_expert\",\n",
    "    step7_dir / \"thresholds_curve_vision_expert_pilot.csv\",\n",
    ")\n",
    "\n",
    "thr_fusion, f1_fusion = threshold_sweep_from_logits(\n",
    "    logits_fusion_cal,\n",
    "    labels,\n",
    "    \"mm_fusion\",\n",
    "    step7_dir / \"thresholds_curve_mm_fusion_pilot.csv\",\n",
    ")\n",
    "\n",
    "thresholds_summary = {\n",
    "    \"text_expert\": {\"abuse_hate\": thr_text, \"best_f1\": f1_text},\n",
    "    \"vision_expert\": {\"abuse_hate\": thr_vision, \"best_f1\": f1_vision},\n",
    "    \"mm_fusion\": {\"abuse_hate\": thr_fusion, \"best_f1\": f1_fusion},\n",
    "}\n",
    "\n",
    "thr_path = step7_dir / \"thresholds_pilot.json\"\n",
    "with thr_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thresholds_summary, f, indent=2)\n",
    "\n",
    "print(\"Recommended thresholds (pilot):\")\n",
    "for name, info in thresholds_summary.items():\n",
    "    print(name, \"-> threshold =\", info[\"abuse_hate\"], \"best F1 =\", info[\"best_f1\"])\n",
    "\n",
    "print(\"\\nSaved threshold curves and summary to\", step7_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049e4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved detailed error table to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_7/errors_pilot.csv\n",
      "Fusion confusion counts (TP, TN, FP, FN): 2 43 2 3\n",
      "Examples where fusion is correct but both unimodal experts are wrong: 1\n"
     ]
    }
   ],
   "source": [
    "# Error analysis using the fusion model as primary decision maker\n",
    "\n",
    "probs_text = torch.softmax(logits_text_cal, dim=-1).cpu().numpy()\n",
    "probs_vision = torch.softmax(logits_vision_cal, dim=-1).cpu().numpy()\n",
    "probs_fusion = torch.softmax(logits_fusion_cal, dim=-1).cpu().numpy()\n",
    "\n",
    "probs_text_pos = probs_text[:, 1]\n",
    "probs_vision_pos = probs_vision[:, 1]\n",
    "probs_fusion_pos = probs_fusion[:, 1]\n",
    "\n",
    "labels_np = labels.cpu().numpy()\n",
    "\n",
    "preds_text = (probs_text_pos >= thr_text).astype(int)\n",
    "preds_vision = (probs_vision_pos >= thr_vision).astype(int)\n",
    "preds_fusion = (probs_fusion_pos >= thr_fusion).astype(int)\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, ex in enumerate(eval_examples):\n",
    "    label = int(labels_np[i])\n",
    "    pt = float(probs_text_pos[i])\n",
    "    pv = float(probs_vision_pos[i])\n",
    "    pf = float(probs_fusion_pos[i])\n",
    "    yt = int(preds_text[i])\n",
    "    yv = int(preds_vision[i])\n",
    "    yf = int(preds_fusion[i])\n",
    "\n",
    "    if yf == 1 and label == 1:\n",
    "        err_type = \"TP\"\n",
    "    elif yf == 0 and label == 0:\n",
    "        err_type = \"TN\"\n",
    "    elif yf == 1 and label == 0:\n",
    "        err_type = \"FP\"\n",
    "    else:\n",
    "        err_type = \"FN\"\n",
    "\n",
    "    all_agree = int((yt == yv) and (yv == yf))\n",
    "    fusion_correct_both_wrong = int((yf == label) and (yt != label) and (yv != label))\n",
    "\n",
    "    rows.append({\n",
    "        \"index\": i,\n",
    "        \"text\": ex[\"text\"],\n",
    "        \"label\": label,\n",
    "        \"text_prob\": pt,\n",
    "        \"text_pred\": yt,\n",
    "        \"vision_prob\": pv,\n",
    "        \"vision_pred\": yv,\n",
    "        \"fusion_prob\": pf,\n",
    "        \"fusion_pred\": yf,\n",
    "        \"fusion_error_type\": err_type,\n",
    "        \"all_models_agree\": all_agree,\n",
    "        \"fusion_correct_both_wrong\": fusion_correct_both_wrong,\n",
    "    })\n",
    "\n",
    "errors_path = step7_dir / \"errors_pilot.csv\"\n",
    "with errors_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"index\",\n",
    "            \"text\",\n",
    "            \"label\",\n",
    "            \"text_prob\",\n",
    "            \"text_pred\",\n",
    "            \"vision_prob\",\n",
    "            \"vision_pred\",\n",
    "            \"fusion_prob\",\n",
    "            \"fusion_pred\",\n",
    "            \"fusion_error_type\",\n",
    "            \"all_models_agree\",\n",
    "            \"fusion_correct_both_wrong\",\n",
    "        ],\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "# Print a brief summary of fusion error types\n",
    "fusion_preds = preds_fusion\n",
    "fp = int(np.logical_and(fusion_preds == 1, labels_np == 0).sum())\n",
    "fn = int(np.logical_and(fusion_preds == 0, labels_np == 1).sum())\n",
    "tp = int(np.logical_and(fusion_preds == 1, labels_np == 1).sum())\n",
    "tn = int(np.logical_and(fusion_preds == 0, labels_np == 0).sum())\n",
    "\n",
    "print(\"Saved detailed error table to\", errors_path)\n",
    "print(\"Fusion confusion counts (TP, TN, FP, FN):\", tp, tn, fp, fn)\n",
    "print(\"Examples where fusion is correct but both unimodal experts are wrong:\", int(sum(r[\"fusion_correct_both_wrong\"] for r in rows)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
