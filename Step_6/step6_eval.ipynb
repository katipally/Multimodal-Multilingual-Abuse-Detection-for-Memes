{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7b76ff",
   "metadata": {},
   "source": [
    "# Step 6: Evaluation and Calibration (Pilot)\n",
    "\n",
    "This notebook evaluates the **text expert**, **vision expert**, and **fusion model** on the pilot shard from Step 3.\n",
    "\n",
    "You can later adapt the same code to full train/val/test splits once Step 3 has been run on all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9135d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: webdataset in /opt/anaconda3/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.22)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: braceexpand in /opt/anaconda3/lib/python3.12/site-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Step 6 (run once per environment).\n",
    "# You can skip this cell if everything is already installed.\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Core libraries\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# NLP / vision / datasets / training utilities\n",
    "%pip install transformers datasets webdataset accelerate timm sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dfe57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Shard pattern: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_3/shards/train/shard-000000.tar\n",
      "Text expert dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/text_expert\n",
      "Vision expert dir: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/vision_expert\n",
      "Fusion model path: /Users/yashwanthreddy/Documents/GitHub/DL_Proj/models/mm_fusion/fusion_model.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "\n",
    "# Detect project root so this works whether you start Jupyter in the repo root\n",
    "# or from inside Step_6/.\n",
    "cwd = Path.cwd().resolve()\n",
    "if (cwd / \"Step_3\").is_dir():\n",
    "    root = cwd\n",
    "else:\n",
    "    root = cwd.parent\n",
    "\n",
    "step3 = root / \"Step_3\"\n",
    "shards_dir = step3 / \"shards\" / \"train\"\n",
    "shard_pattern = str(shards_dir / \"shard-000000.tar\")  # 50-example pilot shard\n",
    "\n",
    "models_root = root / \"models\"\n",
    "text_expert_dir = models_root / \"text_expert\"\n",
    "vision_expert_dir = models_root / \"vision_expert\"\n",
    "mm_fusion_dir = models_root / \"mm_fusion\"\n",
    "mm_fusion_path = mm_fusion_dir / \"fusion_model.pt\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "print(\"Shard pattern:\", shard_pattern)\n",
    "print(\"Text expert dir:\", text_expert_dir)\n",
    "print(\"Vision expert dir:\", vision_expert_dir)\n",
    "print(\"Fusion model path:\", mm_fusion_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8da93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 evaluation examples.\n"
     ]
    }
   ],
   "source": [
    "def make_eval_examples(shard_pattern: str, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create a list of {text, image, label} from WebDataset shards for evaluation.\"\"\"\n",
    "\n",
    "    ds = (\n",
    "        wds.WebDataset(shard_pattern, shardshuffle=False)\n",
    "        .decode(\"pil\")\n",
    "        .to_tuple(\"txt\", \"png\", \"json\")\n",
    "    )\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for text_obj, img, meta_obj in ds:\n",
    "        # Decode text\n",
    "        if isinstance(text_obj, (bytes, bytearray)):\n",
    "            text = text_obj.decode(\"utf-8\", errors=\"replace\")\n",
    "        else:\n",
    "            text = str(text_obj)\n",
    "\n",
    "        # Decode metadata\n",
    "        if isinstance(meta_obj, (bytes, bytearray)):\n",
    "            meta = json.loads(meta_obj.decode(\"utf-8\"))\n",
    "        else:\n",
    "            meta = meta_obj\n",
    "\n",
    "        labels = (meta or {}).get(\"labels\", {})\n",
    "        y = labels.get(\"abuse_hate\")\n",
    "        if y is None:\n",
    "            continue\n",
    "\n",
    "        out.append({\n",
    "            \"text\": text,\n",
    "            \"image\": img,\n",
    "            \"label\": int(y),\n",
    "        })\n",
    "\n",
    "        if max_samples is not None and len(out) >= max_samples:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "eval_examples = make_eval_examples(shard_pattern, max_samples=1000)\n",
    "print(f\"Loaded {len(eval_examples)} evaluation examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32efe4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 7\n"
     ]
    }
   ],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, examples: List[Dict[str, Any]]):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "def collate_batch(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    return {\"texts\": texts, \"images\": images, \"labels\": labels}\n",
    "\n",
    "\n",
    "dataset_eval = EvalDataset(eval_examples)\n",
    "loader_eval = DataLoader(dataset_eval, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(\"Batches per epoch:\", len(loader_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964697c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds: np.ndarray, labels: np.ndarray) -> float:\n",
    "    return float((preds == labels).mean()) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_macro_f1(preds: np.ndarray, labels: np.ndarray, num_classes: int = 2) -> float:\n",
    "    f1s: List[float] = []\n",
    "    for c in range(num_classes):\n",
    "        tp = np.logical_and(preds == c, labels == c).sum()\n",
    "        fp = np.logical_and(preds == c, labels != c).sum()\n",
    "        fn = np.logical_and(preds != c, labels == c).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s)) if f1s else 0.0\n",
    "\n",
    "\n",
    "def compute_brier_score(probs_pos: np.ndarray, labels: np.ndarray) -> float:\n",
    "    # probs_pos: probability of class 1\n",
    "    return float(np.mean((probs_pos - labels) ** 2)) if len(labels) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_ece(probs_pos: np.ndarray, labels: np.ndarray, num_bins: int = 10) -> float:\n",
    "    # Simple Expected Calibration Error for binary classification.\n",
    "    bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "    ece = 0.0\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        mask = (probs_pos >= bins[i]) & (probs_pos < bins[i + 1])\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        bin_conf = probs_pos[mask].mean()\n",
    "        bin_acc = (labels[mask] == (probs_pos[mask] >= 0.5)).mean()\n",
    "        ece += (mask.sum() / n) * abs(bin_conf - bin_acc)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def summarize_metrics(logits: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    probs_pos = probs[:, 1]\n",
    "\n",
    "    acc = compute_accuracy(preds, labels_np)\n",
    "    macro_f1 = compute_macro_f1(preds, labels_np, num_classes=2)\n",
    "    brier = compute_brier_score(probs_pos, labels_np)\n",
    "    ece = compute_ece(probs_pos, labels_np, num_bins=10)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"brier\": brier,\n",
    "        \"ece\": ece,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899ebaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers / processors used by the experts\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_expert_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_expert_dir)\n",
    "\n",
    "\n",
    "def evaluate_text_expert(loader: DataLoader) -> Dict[str, float]:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(text_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = text_tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    metrics = summarize_metrics(logits_cat, labels_cat)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_vision_expert(loader: DataLoader) -> Dict[str, float]:\n",
    "    model = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc = image_processor(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc[\"pixel_values\"].to(device)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    metrics = summarize_metrics(logits_cat, labels_cat)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376bf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion model definition and evaluation\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: nn.Module,\n",
    "        vision_encoder: nn.Module,\n",
    "        t_dim: int,\n",
    "        v_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(t_dim + v_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            text_out = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            if hasattr(text_out, \"pooler_output\") and text_out.pooler_output is not None:\n",
    "                t_repr = text_out.pooler_output\n",
    "            else:\n",
    "                t_repr = text_out.last_hidden_state[:, 0, :]\n",
    "\n",
    "            vision_out = self.vision_encoder(pixel_values=pixel_values)\n",
    "            v_repr = vision_out.logits\n",
    "\n",
    "        h = torch.cat([t_repr, v_repr], dim=-1)\n",
    "        logits = self.mlp(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def load_fusion_model() -> FusionModel:\n",
    "    # Recreate encoders as in Step 5\n",
    "    text_encoder = AutoModel.from_pretrained(text_expert_dir)\n",
    "    vision_encoder = AutoModelForImageClassification.from_pretrained(vision_expert_dir)\n",
    "\n",
    "    text_encoder.to(device)\n",
    "    vision_encoder.to(device)\n",
    "\n",
    "    for p in text_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in vision_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    t_dim = text_encoder.config.hidden_size\n",
    "    v_dim = vision_encoder.config.num_labels\n",
    "\n",
    "    fusion_hidden = 512\n",
    "    num_labels = 2\n",
    "\n",
    "    model = FusionModel(\n",
    "        text_encoder=text_encoder,\n",
    "        vision_encoder=vision_encoder,\n",
    "        t_dim=t_dim,\n",
    "        v_dim=v_dim,\n",
    "        hidden_dim=fusion_hidden,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    state_dict = torch.load(mm_fusion_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_fusion_model(loader: DataLoader) -> Dict[str, float]:\n",
    "    model = load_fusion_model()\n",
    "\n",
    "    all_logits: List[torch.Tensor] = []\n",
    "    all_labels: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            texts = batch[\"texts\"]\n",
    "            images = batch[\"images\"]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            enc_text = text_tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc_text = {k: v.to(device) for k, v in enc_text.items()}\n",
    "\n",
    "            enc_img = image_processor(images=images, return_tensors=\"pt\")\n",
    "            pixel_values = enc_img[\"pixel_values\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=enc_text[\"input_ids\"],\n",
    "                attention_mask=enc_text[\"attention_mask\"],\n",
    "                pixel_values=pixel_values,\n",
    "            )\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits_cat = torch.cat(all_logits, dim=0)\n",
    "    labels_cat = torch.cat(all_labels, dim=0)\n",
    "    metrics = summarize_metrics(logits_cat, labels_cat)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf2cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating text expert...\n",
      "Text expert metrics: {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.12276667188716842, 'ece': 0.6138291072845459}\n",
      "\n",
      "Evaluating vision expert...\n",
      "Vision expert metrics: {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.09003111768167933, 'ece': 0.8844988291338086}\n",
      "\n",
      "Evaluating fusion model...\n",
      "Fusion model metrics: {'accuracy': 0.9, 'macro_f1': 0.4736842105263158, 'brier': 0.08550730560385912, 'ece': 0.7489281076192857}\n",
      "\n",
      "Saved pilot evaluation metrics to /Users/yashwanthreddy/Documents/GitHub/DL_Proj/Step_6/results_pilot.json\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for all three models and save metrics to JSON\n",
    "\n",
    "results: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "print(\"Evaluating text expert...\")\n",
    "results[\"text_expert\"] = evaluate_text_expert(loader_eval)\n",
    "print(\"Text expert metrics:\", results[\"text_expert\"])\n",
    "\n",
    "print(\"\\nEvaluating vision expert...\")\n",
    "results[\"vision_expert\"] = evaluate_vision_expert(loader_eval)\n",
    "print(\"Vision expert metrics:\", results[\"vision_expert\"])\n",
    "\n",
    "print(\"\\nEvaluating fusion model...\")\n",
    "results[\"mm_fusion\"] = evaluate_fusion_model(loader_eval)\n",
    "print(\"Fusion model metrics:\", results[\"mm_fusion\"])\n",
    "\n",
    "# Save to JSON\n",
    "step6_results_dir = Path(\"Step_6\") if (Path.cwd() / \"Step_6\").is_dir() else root / \"Step_6\"\n",
    "step6_results_dir.mkdir(exist_ok=True)\n",
    "results_path = step6_results_dir / \"results_pilot.json\"\n",
    "with results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved pilot evaluation metrics to\", results_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
